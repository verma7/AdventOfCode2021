{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxcEDSI0eX6quecAjnPNbi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/verma7/AdventOfCode2021/blob/master/RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vjd1M1WkWU5R"
      },
      "outputs": [],
      "source": [
        "# RL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-agents[reverb]\n",
        "!pip install tf-keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-0x0yNuWqi0",
        "outputId": "8dc29418-ca5b-4365-f596-f4a666c6f736"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-agents[reverb]\n",
            "  Downloading tf_agents-0.19.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (3.1.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents[reverb])\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/624.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (11.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (1.17.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (4.25.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from tf-agents[reverb]) (1.17.2)\n",
            "Collecting typing-extensions==4.5.0 (from tf-agents[reverb])\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pygame==2.1.3 (from tf-agents[reverb])\n",
            "  Downloading pygame-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting tensorflow-probability~=0.23.0 (from tf-agents[reverb])\n",
            "  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting rlds (from tf-agents[reverb])\n",
            "  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting dm-reverb~=0.14.0 (from tf-agents[reverb])\n",
            "  Downloading dm_reverb-0.14.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Collecting tensorflow~=2.15.0 (from tf-agents[reverb])\n",
            "  Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (0.1.9)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.11/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow~=2.15.0->tf-agents[reverb])\n",
            "  Downloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.5.0)\n",
            "Collecting wrapt>=1.11.1 (from tf-agents[reverb])\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.70.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow~=2.15.0->tf-agents[reverb])\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow~=2.15.0->tf-agents[reverb])\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow~=2.15.0->tf-agents[reverb])\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.23.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-agents[reverb]) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.1.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->dm-reverb~=0.14.0->tf-agents[reverb]) (25.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from portpicker->dm-reverb~=0.14.0->tf-agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.2.2)\n",
            "Downloading pygame-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading dm_reverb-0.14.0-cp311-cp311-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_agents-0.19.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697715 sha256=fa605da1d776bbae132b1fd6b7895dcdc1a72dcd36a403684788db5a9e5050e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/19/ce/d2b762b6d61115bf0b4260ca59650ba2d55d49f34f61e095f6\n",
            "Successfully built gym\n",
            "Installing collected packages: wrapt, typing-extensions, tensorflow-estimator, rlds, pygame, ml-dtypes, keras, gym, tensorflow-probability, dm-reverb, tf-agents, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.1\n",
            "    Uninstalling pygame-2.6.1:\n",
            "      Successfully uninstalled pygame-2.6.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.25.0\n",
            "    Uninstalling tensorflow-probability-0.25.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.25.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "altair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "sqlalchemy 2.0.38 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.1 which is incompatible.\n",
            "pydantic 2.10.6 requires typing-extensions>=4.12.2, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "langchain-core 0.3.35 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.1 which is incompatible.\n",
            "typeguard 4.4.1 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.27.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "openai 1.61.1 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dm-reverb-0.14.0 gym-0.23.0 keras-2.15.0 ml-dtypes-0.3.2 pygame-2.1.3 rlds-0.1.8 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 tensorflow-probability-0.23.0 tf-agents-0.19.0 typing-extensions-4.5.0 wrapt-1.14.1\n",
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting tensorflow<2.19,>=2.18 (from tf-keras)\n",
            "  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18->tf-keras)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
            "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
            "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
            "  Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
            "Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.3.2\n",
            "    Uninstalling ml-dtypes-0.3.2:\n",
            "      Successfully uninstalled ml-dtypes-0.3.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.1\n",
            "    Uninstalling tensorflow-2.15.1:\n",
            "      Successfully uninstalled tensorflow-2.15.1\n",
            "Successfully installed keras-3.8.0 ml-dtypes-0.4.1 tensorboard-2.18.0 tensorflow-2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ],
      "metadata": {
        "id": "MT59_Qw0XPiQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install galois"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFA9JVKfaQ4p",
        "outputId": "653f17b6-8936-4e00-ce25-4ba88fa20b23"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting galois\n",
            "  Downloading galois-0.4.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from galois) (1.26.4)\n",
            "Requirement already satisfied: numba<0.62,>=0.55 in /usr/local/lib/python3.11/dist-packages (from galois) (0.61.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from galois) (4.5.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba<0.62,>=0.55->galois) (0.44.0)\n",
            "Downloading galois-0.4.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: galois\n",
            "Successfully installed galois-0.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import galois\n",
        "import pandas as pd\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.drivers import dynamic_episode_driver\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "\n",
        "from tf_agents import specs\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "\n",
        "from tf_agents.train.utils import strategy_utils\n"
      ],
      "metadata": {
        "id": "0CtpTB0KXSUx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_ones(w):\n",
        "    \"Returns a vector containing the number of ones in the bit representation in each element of the GF(2^w) field.\"\n",
        "    N = 2 ** w\n",
        "    GF = galois.GF(N)\n",
        "    num_ones = np.zeros(N)\n",
        "    for i in range(N):\n",
        "        for j in range(w):\n",
        "            v = np.multiply(GF(i), GF(2 ** j))\n",
        "            num_ones[i] += bin(v).count(\"1\")\n",
        "    return num_ones\n",
        "\n",
        "def count_ones(GC, n, m, ones):\n",
        "    sum_ones = 0\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            sum_ones += ones[int(GC[i][j])]\n",
        "    return sum_ones\n",
        "\n",
        "def swap(x, y, state_x, state_y, state):\n",
        "  xi = 0\n",
        "  for i in range(len(state)):\n",
        "    if state[i] == state_x:\n",
        "      if xi == x:\n",
        "        break\n",
        "      xi += 1\n",
        "\n",
        "  xj = 0\n",
        "  for j in range(len(state)):\n",
        "    if state[j] == state_y:\n",
        "      if xj == y:\n",
        "        break\n",
        "      xj += 1\n",
        "\n",
        "  tmp = state[i]\n",
        "  state[i] = state[j]\n",
        "  state[j] = tmp\n",
        "\n",
        "def normalize_bitmatrix(GF, GC, m, ONES):\n",
        "  # Make the first row 1s by dividing all the columns by its first element.\n",
        "  M = np.zeros((m, m))\n",
        "  for i in range(m):\n",
        "      for j in range(m):\n",
        "          if GC[0][j] == 0:\n",
        "              continue\n",
        "          M[i][j] = np.divide(GF(int(GC[i][j])), GF(int(GC[0][j])))\n",
        "#    print(M)\n",
        "\n",
        "  # For each row, pick one of the elements that minimizes the number of ones and divide the row by it.\n",
        "  for i in range(1, m):\n",
        "      min_ones = 1e8\n",
        "      best_C = None\n",
        "\n",
        "      for j in range(m):\n",
        "          if M[i][j] == 0:\n",
        "              continue\n",
        "\n",
        "          C = np.zeros(m)\n",
        "          sum_ones = 0\n",
        "          C_ones = []\n",
        "          for k in range(m):\n",
        "              C[k] = np.divide(GF(int(M[i][k])), GF(int(M[i][j])))\n",
        "              sum_ones += ONES[int(C[k])]\n",
        "              C_ones.append(ONES[int(C[k])])\n",
        "          if sum_ones < min_ones:\n",
        "              min_ones = sum_ones\n",
        "              best_C = C\n",
        "      for j in range(m):\n",
        "          if best_C is not None:\n",
        "              M[i][j] = best_C[j]\n",
        "  return M\n",
        "\n",
        "def good_cauchy_matrix(GF, m, N, state):\n",
        "  GC = np.zeros((m, m))\n",
        "  x = 0\n",
        "  y = 0\n",
        "  for i in range(N):\n",
        "      if state[i] == 0:\n",
        "        y = 0\n",
        "        for j in range(N):\n",
        "            if state[j] == 1:\n",
        "              sum = GF(i) + GF(j)\n",
        "              if sum == 0:\n",
        "                  continue\n",
        "              GC[x][y] = sum ** -1\n",
        "              y += 1\n",
        "        x += 1\n",
        "  return GC\n",
        "\n",
        "def simulate_action(action, m, N, state, GF, ONES):\n",
        "  max_action = m * m + 2 * m * N - 1\n",
        "  if action < m * m:\n",
        "    x = action // m\n",
        "    y = action % m\n",
        "    swap(x, y, 0, 1, state)\n",
        "  elif action < m * m + m * N:\n",
        "    denom = m * N\n",
        "    offset = m * m\n",
        "    x = (action - offset) // denom\n",
        "    y = (action - offset) % denom\n",
        "    swap(x, y, 0, 2, state)\n",
        "  elif action < m * m + 2 * m * N:\n",
        "    denom = m * N\n",
        "    offset = m * m + m * N\n",
        "    x = (action - offset) // denom\n",
        "    y = (action - offset) % denom\n",
        "    swap(x, y, 1, 2, state)\n",
        "  else:\n",
        "    raise ValueError(f'`action` should be between [0, {max_action}), but is {action}')\n",
        "\n",
        "  GC = good_cauchy_matrix(GF, m, N, state)\n",
        "  NGC = normalize_bitmatrix(GF, GC, m, ONES)\n",
        "  return count_ones(NGC, m, m, ONES)\n",
        "\n",
        "def random_weighted_action(m, N, state, num_ones):\n",
        "  \"\"\"Returns an action\"\"\"\n",
        "  weights = np.zeros(m * m + 2 * m * N)\n",
        "  GF = galois.GF(N)\n",
        "  ONES = get_num_ones(N)\n",
        "\n",
        "  best_action = 0\n",
        "  best_reward = 0\n",
        "  for action in range(len(weights)):\n",
        "    state_copy = state.copy()\n",
        "    weights[action] = num_ones - simulate_action(action, m, N, state_copy, GF, ONES)\n",
        "    print(f'action = {action}, reward = {weights[action]}, prev num_ones = {num_ones}')\n",
        "    if weights[action] > best_reward:\n",
        "      best_reward = weights[action]\n",
        "      best_action = action\n",
        "  return best_action"
      ],
      "metadata": {
        "id": "r9Fwcjv-aJQI"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CauchyEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, W, m, vec, num_ones_list):\n",
        "    self._W = W\n",
        "    self._N = 2 ** self._W\n",
        "    self._m = m\n",
        "    self._GF = galois.GF(self._N)\n",
        "    self._ONES = get_num_ones(self._W)\n",
        "    self._num_ones_list = num_ones_list\n",
        "\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int64, minimum=0, maximum=self._m * self._m + 2 * self._m * self._N - 1, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(self._N, ), dtype=np.int32, minimum=0, maximum=2, name='observation')\n",
        "    self._initialize(vec)\n",
        "\n",
        "  def _initialize(self, vec):\n",
        "    self._best_num_ones = 1e8\n",
        "    self._vec = vec\n",
        "    self._state = [0] * self._N\n",
        "    for i in self._vec[:self._m]:\n",
        "        self._state[i] = 0\n",
        "    for i in self._vec[self._m:2*self._m]:\n",
        "        self._state[i] = 1\n",
        "    for i in self._vec[2*self._m:]:\n",
        "        self._state[i] = 2\n",
        "    self._GC = good_cauchy_matrix(self._GF, self._m, self._N, self._state)\n",
        "    NGC = normalize_bitmatrix(self._GF, self._GC, self._m, self._ONES)\n",
        "    self._num_ones = count_ones(NGC, self._m, self._m, self._ONES)\n",
        "    self._best_num_ones = self._num_ones\n",
        "    self._time_steps_without_improvement = 0\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._initialize(self._vec)\n",
        "    return ts.restart(np.array(self._state, dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):\n",
        "    new_num_ones = simulate_action(action, self._m, self._N, self._state, self._GF, self._ONES)\n",
        "    reward = self._num_ones - new_num_ones\n",
        "    self._num_ones = new_num_ones\n",
        "    self._num_ones_list.append(new_num_ones)\n",
        "    if new_num_ones < self._best_num_ones:\n",
        "      self._best_num_ones = new_num_ones\n",
        "      # print(f'Best num ones = {new_num_ones}')\n",
        "      self._time_steps_without_improvement = 0\n",
        "    else:\n",
        "      self._time_steps_without_improvement += 1\n",
        "    # print(f'Time steps without improvement: {self._time_steps_without_improvement}, best num ones = {self._best_num_ones}')\n",
        "    if self._time_steps_without_improvement == 10:\n",
        "      self._time_steps_without_improvement = 0\n",
        "      self._initialize(self._vec)\n",
        "      return ts.termination(np.array(self._state, dtype=np.int32), reward)\n",
        "    else:\n",
        "      return ts.transition(np.array(self._state, dtype=np.int32), reward=reward, discount=1.0)"
      ],
      "metadata": {
        "id": "BXCAiZLnXfF4"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = 4\n",
        "m = 4\n",
        "N = 2 ** w\n",
        "vec = [ i for i in range(N)]\n",
        "random.shuffle(vec)\n",
        "\n",
        "\n",
        "max_episodes = 10\n",
        "num_episodes = 0\n",
        "\n",
        "num_ones = []\n",
        "env = CauchyEnv(w, m, vec, num_ones)\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "while (num_episodes < max_episodes):\n",
        "  time_step = tf_env.reset()\n",
        "  num_episodes += 1\n",
        "  num_ones.clear()\n",
        "\n",
        "  while not time_step.is_last():\n",
        "    action = random_weighted_action(env._m, env._N, env._state, env._num_ones)\n",
        "    print(f\"action = {action}\")\n",
        "    time_step = tf_env.step(action)\n",
        "\n",
        "  if num_ones:\n",
        "    print(num_ones)\n",
        "    print(f'Min num_ones: {np.min(num_ones)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6V0foyl2x1BY",
        "outputId": "a1932177-d484-484e-c37e-38900fddcc8c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0, reward = -281.0, prev num_ones = 101.0\n",
            "action = 1, reward = -323.0, prev num_ones = 101.0\n",
            "action = 2, reward = -308.0, prev num_ones = 101.0\n",
            "action = 3, reward = -373.0, prev num_ones = 101.0\n",
            "action = 4, reward = -301.0, prev num_ones = 101.0\n",
            "action = 5, reward = -334.0, prev num_ones = 101.0\n",
            "action = 6, reward = -372.0, prev num_ones = 101.0\n",
            "action = 7, reward = -307.0, prev num_ones = 101.0\n",
            "action = 8, reward = -306.0, prev num_ones = 101.0\n",
            "action = 9, reward = -327.0, prev num_ones = 101.0\n",
            "action = 10, reward = -357.0, prev num_ones = 101.0\n",
            "action = 11, reward = -311.0, prev num_ones = 101.0\n",
            "action = 12, reward = -294.0, prev num_ones = 101.0\n",
            "action = 13, reward = -317.0, prev num_ones = 101.0\n",
            "action = 14, reward = -299.0, prev num_ones = 101.0\n",
            "action = 15, reward = -311.0, prev num_ones = 101.0\n",
            "action = 16, reward = -299.0, prev num_ones = 101.0\n",
            "action = 17, reward = -295.0, prev num_ones = 101.0\n",
            "action = 18, reward = -344.0, prev num_ones = 101.0\n",
            "action = 19, reward = -301.0, prev num_ones = 101.0\n",
            "action = 20, reward = -344.0, prev num_ones = 101.0\n",
            "action = 21, reward = -330.0, prev num_ones = 101.0\n",
            "action = 22, reward = -313.0, prev num_ones = 101.0\n",
            "action = 23, reward = -343.0, prev num_ones = 101.0\n",
            "action = 24, reward = -343.0, prev num_ones = 101.0\n",
            "action = 25, reward = -343.0, prev num_ones = 101.0\n",
            "action = 26, reward = -343.0, prev num_ones = 101.0\n",
            "action = 27, reward = -343.0, prev num_ones = 101.0\n",
            "action = 28, reward = -343.0, prev num_ones = 101.0\n",
            "action = 29, reward = -343.0, prev num_ones = 101.0\n",
            "action = 30, reward = -343.0, prev num_ones = 101.0\n",
            "action = 31, reward = -343.0, prev num_ones = 101.0\n",
            "action = 32, reward = -343.0, prev num_ones = 101.0\n",
            "action = 33, reward = -343.0, prev num_ones = 101.0\n",
            "action = 34, reward = -343.0, prev num_ones = 101.0\n",
            "action = 35, reward = -343.0, prev num_ones = 101.0\n",
            "action = 36, reward = -343.0, prev num_ones = 101.0\n",
            "action = 37, reward = -343.0, prev num_ones = 101.0\n",
            "action = 38, reward = -343.0, prev num_ones = 101.0\n",
            "action = 39, reward = -343.0, prev num_ones = 101.0\n",
            "action = 40, reward = -343.0, prev num_ones = 101.0\n",
            "action = 41, reward = -343.0, prev num_ones = 101.0\n",
            "action = 42, reward = -343.0, prev num_ones = 101.0\n",
            "action = 43, reward = -343.0, prev num_ones = 101.0\n",
            "action = 44, reward = -343.0, prev num_ones = 101.0\n",
            "action = 45, reward = -343.0, prev num_ones = 101.0\n",
            "action = 46, reward = -343.0, prev num_ones = 101.0\n",
            "action = 47, reward = -343.0, prev num_ones = 101.0\n",
            "action = 48, reward = -343.0, prev num_ones = 101.0\n",
            "action = 49, reward = -343.0, prev num_ones = 101.0\n",
            "action = 50, reward = -343.0, prev num_ones = 101.0\n",
            "action = 51, reward = -343.0, prev num_ones = 101.0\n",
            "action = 52, reward = -343.0, prev num_ones = 101.0\n",
            "action = 53, reward = -343.0, prev num_ones = 101.0\n",
            "action = 54, reward = -343.0, prev num_ones = 101.0\n",
            "action = 55, reward = -343.0, prev num_ones = 101.0\n",
            "action = 56, reward = -343.0, prev num_ones = 101.0\n",
            "action = 57, reward = -343.0, prev num_ones = 101.0\n",
            "action = 58, reward = -343.0, prev num_ones = 101.0\n",
            "action = 59, reward = -343.0, prev num_ones = 101.0\n",
            "action = 60, reward = -343.0, prev num_ones = 101.0\n",
            "action = 61, reward = -343.0, prev num_ones = 101.0\n",
            "action = 62, reward = -343.0, prev num_ones = 101.0\n",
            "action = 63, reward = -343.0, prev num_ones = 101.0\n",
            "action = 64, reward = -343.0, prev num_ones = 101.0\n",
            "action = 65, reward = -343.0, prev num_ones = 101.0\n",
            "action = 66, reward = -343.0, prev num_ones = 101.0\n",
            "action = 67, reward = -343.0, prev num_ones = 101.0\n",
            "action = 68, reward = -343.0, prev num_ones = 101.0\n",
            "action = 69, reward = -343.0, prev num_ones = 101.0\n",
            "action = 70, reward = -343.0, prev num_ones = 101.0\n",
            "action = 71, reward = -343.0, prev num_ones = 101.0\n",
            "action = 72, reward = -343.0, prev num_ones = 101.0\n",
            "action = 73, reward = -343.0, prev num_ones = 101.0\n",
            "action = 74, reward = -343.0, prev num_ones = 101.0\n",
            "action = 75, reward = -343.0, prev num_ones = 101.0\n",
            "action = 76, reward = -343.0, prev num_ones = 101.0\n",
            "action = 77, reward = -343.0, prev num_ones = 101.0\n",
            "action = 78, reward = -343.0, prev num_ones = 101.0\n",
            "action = 79, reward = -343.0, prev num_ones = 101.0\n",
            "action = 80, reward = -333.0, prev num_ones = 101.0\n",
            "action = 81, reward = -303.0, prev num_ones = 101.0\n",
            "action = 82, reward = -330.0, prev num_ones = 101.0\n",
            "action = 83, reward = -297.0, prev num_ones = 101.0\n",
            "action = 84, reward = -327.0, prev num_ones = 101.0\n",
            "action = 85, reward = -343.0, prev num_ones = 101.0\n",
            "action = 86, reward = -354.0, prev num_ones = 101.0\n",
            "action = 87, reward = -315.0, prev num_ones = 101.0\n",
            "action = 88, reward = -315.0, prev num_ones = 101.0\n",
            "action = 89, reward = -315.0, prev num_ones = 101.0\n",
            "action = 90, reward = -315.0, prev num_ones = 101.0\n",
            "action = 91, reward = -315.0, prev num_ones = 101.0\n",
            "action = 92, reward = -315.0, prev num_ones = 101.0\n",
            "action = 93, reward = -315.0, prev num_ones = 101.0\n",
            "action = 94, reward = -315.0, prev num_ones = 101.0\n",
            "action = 95, reward = -315.0, prev num_ones = 101.0\n",
            "action = 96, reward = -315.0, prev num_ones = 101.0\n",
            "action = 97, reward = -315.0, prev num_ones = 101.0\n",
            "action = 98, reward = -315.0, prev num_ones = 101.0\n",
            "action = 99, reward = -315.0, prev num_ones = 101.0\n",
            "action = 100, reward = -315.0, prev num_ones = 101.0\n",
            "action = 101, reward = -315.0, prev num_ones = 101.0\n",
            "action = 102, reward = -315.0, prev num_ones = 101.0\n",
            "action = 103, reward = -315.0, prev num_ones = 101.0\n",
            "action = 104, reward = -315.0, prev num_ones = 101.0\n",
            "action = 105, reward = -315.0, prev num_ones = 101.0\n",
            "action = 106, reward = -315.0, prev num_ones = 101.0\n",
            "action = 107, reward = -315.0, prev num_ones = 101.0\n",
            "action = 108, reward = -315.0, prev num_ones = 101.0\n",
            "action = 109, reward = -315.0, prev num_ones = 101.0\n",
            "action = 110, reward = -315.0, prev num_ones = 101.0\n",
            "action = 111, reward = -315.0, prev num_ones = 101.0\n",
            "action = 112, reward = -315.0, prev num_ones = 101.0\n",
            "action = 113, reward = -315.0, prev num_ones = 101.0\n",
            "action = 114, reward = -315.0, prev num_ones = 101.0\n",
            "action = 115, reward = -315.0, prev num_ones = 101.0\n",
            "action = 116, reward = -315.0, prev num_ones = 101.0\n",
            "action = 117, reward = -315.0, prev num_ones = 101.0\n",
            "action = 118, reward = -315.0, prev num_ones = 101.0\n",
            "action = 119, reward = -315.0, prev num_ones = 101.0\n",
            "action = 120, reward = -315.0, prev num_ones = 101.0\n",
            "action = 121, reward = -315.0, prev num_ones = 101.0\n",
            "action = 122, reward = -315.0, prev num_ones = 101.0\n",
            "action = 123, reward = -315.0, prev num_ones = 101.0\n",
            "action = 124, reward = -315.0, prev num_ones = 101.0\n",
            "action = 125, reward = -315.0, prev num_ones = 101.0\n",
            "action = 126, reward = -315.0, prev num_ones = 101.0\n",
            "action = 127, reward = -315.0, prev num_ones = 101.0\n",
            "action = 128, reward = -315.0, prev num_ones = 101.0\n",
            "action = 129, reward = -315.0, prev num_ones = 101.0\n",
            "action = 130, reward = -315.0, prev num_ones = 101.0\n",
            "action = 131, reward = -315.0, prev num_ones = 101.0\n",
            "action = 132, reward = -315.0, prev num_ones = 101.0\n",
            "action = 133, reward = -315.0, prev num_ones = 101.0\n",
            "action = 134, reward = -315.0, prev num_ones = 101.0\n",
            "action = 135, reward = -315.0, prev num_ones = 101.0\n",
            "action = 136, reward = -315.0, prev num_ones = 101.0\n",
            "action = 137, reward = -315.0, prev num_ones = 101.0\n",
            "action = 138, reward = -315.0, prev num_ones = 101.0\n",
            "action = 139, reward = -315.0, prev num_ones = 101.0\n",
            "action = 140, reward = -315.0, prev num_ones = 101.0\n",
            "action = 141, reward = -315.0, prev num_ones = 101.0\n",
            "action = 142, reward = -315.0, prev num_ones = 101.0\n",
            "action = 143, reward = -315.0, prev num_ones = 101.0\n",
            "action = 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-351406ddda17>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_weighted_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_ones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"action = {action}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-69ac7678ce6f>\u001b[0m in \u001b[0;36mrandom_weighted_action\u001b[0;34m(m, N, state, num_ones)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0mGF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgalois\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m   \u001b[0mONES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_num_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-69ac7678ce6f>\u001b[0m in \u001b[0;36mget_num_ones\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mnum_ones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnum_ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/galois/_fields/_array.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, x, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;34m\"and instantiate an array using `x = GF(array_like)`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             )\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     def __init__(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/galois/_domains/_array.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, x, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Perform view without verification since the elements were verified in _verify_array_like_types_and_values()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/galois/_domains/_array.py\u001b[0m in \u001b[0;36m_view\u001b[0;34m(cls, array)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mFor\u001b[0m \u001b[0minternal\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0muse\u001b[0m \u001b[0monly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \"\"\"\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_view_without_verification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/galois/_domains/_array.py\u001b[0m in \u001b[0;36m_view_without_verification\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mprev_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_on_view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_on_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_on_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = 4\n",
        "m = 4\n",
        "N = 2 ** w\n",
        "vec = [ i for i in range(N)]\n",
        "random.shuffle(vec)\n",
        "\n",
        "R = np.zeros((N, N))\n",
        "num_ones = []\n",
        "num_actions = m * m + 2 * m * N\n",
        "for i in range(num_actions):\n",
        "  environment = CauchyEnv(w, m, vec, num_ones)\n",
        "  time_step = environment.reset()\n",
        "  time_step = environment.step(i)\n",
        "  R[i//N][i%N] = time_step.reward\n",
        "\n",
        "print(num_ones)\n",
        "for i in range(N):\n",
        "  print(' '.join([f\"{R[i][j]:3.0f}\" for j in range(N)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frzZx7ydhYYa",
        "outputId": "30ba2b9f-fc32-43a6-a200-83b41f669bb3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[103.0, 101.0, 100.0, 98.0, 99.0, 92.0, 107.0, 101.0, 94.0, 96.0, 100.0, 97.0, 99.0, 100.0, 98.0, 106.0, 101.0, 98.0, 97.0, 103.0, 102.0, 99.0, 104.0, 103.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 101.0, 100.0, 105.0, 89.0, 97.0, 98.0, 100.0, 96.0, 97.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0]\n",
            "  2   0  -1  -3  -2  -9   6   0  -7  -5  -1  -4  -2  -1  -3   5\n",
            "  0  -3  -4   2   1  -2   3   2   0   0   0   0   0   0   0   0\n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            " -1   4 -12  -4  -3  -1  -5  -4  -2  -2  -2  -2  -2  -2  -2  -2\n",
            " -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2\n",
            " -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2\n",
            " -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2  -2\n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = 4\n",
        "N = 5\n",
        "vec = [ i for i in range(2 ** W)]\n",
        "random.shuffle(vec)\n",
        "env = CauchyEnv(W, N, vec, num_ones)\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "tf_policy = random_tf_policy.RandomTFPolicy(action_spec=tf_env.action_spec(),\n",
        "                                            time_step_spec=tf_env.time_step_spec())\n",
        "\n",
        "\n",
        "num_episodes = tf_metrics.NumberOfEpisodes()\n",
        "env_steps = tf_metrics.EnvironmentSteps()\n",
        "max_return = tf_metrics.MaxReturnMetric()\n",
        "replay_buffer = []\n",
        "observers = [num_episodes, env_steps, max_return, replay_buffer.append]\n",
        "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "    tf_env, tf_policy, observers, num_episodes=100)\n",
        "\n",
        "# Initial driver.run will reset the environment and initialize the policy.\n",
        "final_time_step, policy_state = driver.run()\n",
        "\n",
        "print('final_time_step', final_time_step)\n",
        "print('Number of Steps: ', env_steps.result().numpy())\n",
        "print('Number of Episodes: ', num_episodes.result().numpy())\n",
        "print('Max Return: ', max_return.result().numpy())\n",
        "print('Min num_ones: ', np.min(num_ones))\n",
        "\n",
        "\n",
        "for epsilon in np.arange(0, 1.1, 0.1):\n",
        "  num_ones = []\n",
        "  environment = CauchyEnv(W, N, vec, num_ones)\n",
        "  tf_env = tf_py_environment.TFPyEnvironment(environment)\n",
        "\n",
        "  q_net = q_network.QNetwork(\n",
        "      tf_env.time_step_spec().observation,\n",
        "      tf_env.action_spec(),\n",
        "      fc_layer_params=(100,))\n",
        "\n",
        "  agent = dqn_agent.DqnAgent(\n",
        "      tf_env.time_step_spec(),\n",
        "      tf_env.action_spec(),\n",
        "      q_network=q_net,\n",
        "      epsilon_greedy=epsilon,\n",
        "      optimizer=tf.compat.v1.train.AdamOptimizer(1e-4))\n",
        "\n",
        "  collect_steps_per_iteration = 100\n",
        "  avg_return_metric = tf_metrics.AverageReturnMetric()\n",
        "  avg_episode_length_metric = tf_metrics.AverageEpisodeLengthMetric()\n",
        "  observers = [] #avg_return_metric, avg_episode_length_metric]\n",
        "  collect_op = dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "    tf_env,\n",
        "    agent.collect_policy,\n",
        "    observers=observers,\n",
        "    num_episodes=collect_steps_per_iteration).run()\n",
        "\n",
        "  print(num_ones)\n",
        "  print(f\"Best num_ones with epsilon greedy={epsilon} is {np.min(num_ones)}\")\n",
        "  # print(f\"Avg return = {avg_return_metric.result().numpy()}\")\n",
        "  # print(f\"Avg episode length = {avg_episode_length_metric.result().numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrP3Jthjt5KS",
        "outputId": "810b4e47-aed0-4610-bf96-14e99bbacd60"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TFDeque.max at 0x7f055ae928e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final_time_step TimeStep(\n",
            "{'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
            " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-5.], dtype=float32)>,\n",
            " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=array([[1, 0, 2, 2, 1, 1, 2, 0, 1, 1, 2, 0, 2, 2, 0, 0]], dtype=int32)>})\n",
            "Number of Steps:  1500\n",
            "Number of Episodes:  100\n",
            "Max Return:  -254.0\n",
            "Min num_ones:  147.0\n",
            "[160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0]\n",
            "Best num_ones with epsilon greedy=0.0 is 155.0\n",
            "[165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 164.0, 164.0, 164.0, 164.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 165.0, 165.0, 165.0, 165.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 165.0, 165.0, 164.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 158.0, 165.0, 165.0, 165.0, 165.0, 165.0, 158.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 158.0, 158.0, 158.0, 154.0, 166.0, 166.0, 166.0, 166.0, 166.0, 166.0, 166.0, 166.0, 166.0, 166.0, 165.0, 165.0, 165.0, 165.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 165.0, 165.0, 165.0, 153.0, 167.0, 167.0, 167.0, 167.0, 167.0, 167.0, 167.0, 167.0, 167.0, 167.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 150.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 161.0, 161.0, 165.0, 165.0, 165.0, 165.0, 165.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 165.0, 165.0, 160.0, 160.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 152.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 155.0, 168.0, 168.0, 168.0, 168.0, 168.0, 155.0, 168.0, 168.0, 168.0, 168.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 152.0, 159.0, 152.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 152.0, 152.0, 159.0, 159.0, 159.0, 163.0, 163.0, 163.0, 163.0, 163.0, 152.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 158.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0]\n",
            "Best num_ones with epsilon greedy=0.1 is 150.0\n",
            "[165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 164.0, 164.0, 164.0, 164.0, 164.0, 160.0, 160.0, 165.0, 160.0, 160.0, 168.0, 168.0, 168.0, 168.0, 168.0, 168.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 161.0, 161.0, 161.0, 164.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 155.0, 155.0, 159.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 169.0, 169.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 160.0, 165.0, 160.0, 160.0, 165.0, 158.0, 163.0, 163.0, 163.0, 158.0, 163.0, 163.0, 158.0, 163.0, 163.0, 163.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 152.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 157.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 160.0, 160.0, 160.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 158.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 159.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 164.0, 164.0, 164.0, 164.0, 164.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 162.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 155.0, 155.0, 155.0, 155.0, 162.0, 166.0, 166.0, 171.0, 171.0, 171.0, 173.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 166.0, 166.0, 166.0, 166.0, 166.0, 166.0, 161.0, 166.0, 166.0, 160.0, 160.0, 157.0, 163.0, 169.0, 169.0, 169.0, 169.0, 169.0, 169.0, 169.0, 169.0, 169.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 165.0, 153.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 154.0, 163.0, 166.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 166.0, 163.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 158.0, 165.0, 165.0, 165.0, 165.0, 158.0, 165.0, 165.0, 165.0, 165.0, 158.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 164.0, 157.0, 164.0, 164.0, 169.0, 169.0, 160.0, 169.0, 169.0, 169.0, 169.0, 169.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 165.0, 159.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 164.0, 171.0, 171.0, 160.0, 160.0, 151.0, 151.0, 153.0, 153.0, 153.0, 153.0, 156.0, 164.0, 164.0, 164.0, 156.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 159.0, 164.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 164.0, 164.0, 164.0, 164.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 157.0, 164.0, 164.0, 164.0, 164.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0]\n",
            "Best num_ones with epsilon greedy=0.2 is 151.0\n",
            "[165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 164.0, 159.0, 164.0, 159.0, 164.0, 159.0, 155.0, 155.0, 162.0, 164.0, 169.0, 161.0, 163.0, 161.0, 163.0, 161.0, 163.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 151.0, 155.0, 164.0, 157.0, 164.0, 157.0, 164.0, 157.0, 164.0, 157.0, 164.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 164.0, 164.0, 161.0, 161.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 164.0, 164.0, 164.0, 164.0, 164.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 159.0, 161.0, 171.0, 171.0, 161.0, 171.0, 165.0, 161.0, 159.0, 159.0, 159.0, 160.0, 160.0, 159.0, 154.0, 157.0, 164.0, 163.0, 160.0, 157.0, 164.0, 157.0, 164.0, 164.0, 164.0, 160.0, 165.0, 160.0, 160.0, 160.0, 159.0, 154.0, 157.0, 157.0, 164.0, 157.0, 164.0, 157.0, 157.0, 168.0, 157.0, 168.0, 160.0, 165.0, 160.0, 160.0, 160.0, 159.0, 164.0, 157.0, 164.0, 157.0, 164.0, 163.0, 160.0, 160.0, 157.0, 164.0, 157.0, 164.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 153.0, 164.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 157.0, 164.0, 157.0, 164.0, 157.0, 164.0, 157.0, 164.0, 159.0, 164.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 161.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 158.0, 159.0, 164.0, 159.0, 164.0, 159.0, 164.0, 157.0, 164.0, 157.0, 164.0, 164.0, 159.0, 164.0, 157.0, 157.0, 159.0, 167.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 162.0, 162.0, 162.0, 158.0, 159.0, 159.0, 156.0, 152.0, 159.0, 159.0, 152.0, 168.0, 152.0, 168.0, 152.0, 168.0, 158.0, 158.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 161.0, 160.0, 160.0, 160.0, 160.0, 164.0, 168.0, 164.0, 168.0, 160.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 158.0, 163.0, 160.0, 170.0, 169.0, 163.0, 163.0, 155.0, 160.0, 155.0, 160.0, 155.0, 160.0, 160.0, 155.0, 156.0, 163.0, 163.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 164.0, 164.0, 165.0, 164.0, 171.0, 164.0, 164.0, 171.0, 164.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 161.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 166.0, 166.0, 166.0, 166.0, 166.0, 166.0, 157.0, 157.0, 157.0, 163.0, 169.0, 169.0, 163.0, 163.0, 169.0, 169.0, 169.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 159.0, 164.0, 157.0, 164.0, 157.0, 164.0, 157.0, 164.0, 157.0, 164.0, 157.0, 164.0, 157.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 159.0, 159.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 158.0, 152.0, 164.0, 154.0, 158.0, 154.0, 164.0, 154.0, 158.0, 157.0, 160.0, 157.0, 160.0, 159.0, 164.0, 159.0, 164.0, 157.0, 164.0, 157.0, 164.0, 157.0, 164.0, 157.0, 164.0, 157.0, 157.0, 164.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 164.0, 153.0, 161.0, 155.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 164.0, 161.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 160.0, 165.0, 160.0, 160.0, 161.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 160.0, 160.0, 151.0, 155.0, 164.0, 157.0, 164.0, 164.0, 157.0, 164.0, 164.0, 157.0, 164.0, 157.0, 162.0, 162.0, 162.0, 162.0, 157.0, 162.0, 162.0, 162.0, 162.0, 162.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 151.0, 155.0, 164.0, 157.0, 164.0, 157.0, 164.0, 164.0, 157.0, 164.0, 164.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 166.0, 165.0, 156.0, 165.0, 165.0, 167.0, 165.0, 156.0, 156.0, 165.0, 156.0, 165.0, 156.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 161.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 159.0, 154.0, 157.0, 168.0, 164.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 164.0, 164.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 153.0, 157.0, 157.0, 153.0, 157.0, 157.0, 153.0, 158.0, 156.0, 153.0, 162.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 164.0, 164.0, 164.0, 164.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 157.0, 164.0, 157.0, 164.0, 157.0, 164.0, 164.0, 157.0, 157.0, 164.0, 157.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 169.0, 169.0, 169.0, 153.0, 167.0, 167.0, 167.0, 167.0, 167.0, 167.0, 153.0, 167.0, 167.0, 162.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 154.0, 163.0, 160.0, 163.0, 163.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 160.0, 160.0, 160.0, 160.0, 158.0, 158.0, 163.0, 163.0, 157.0, 163.0, 163.0, 157.0, 163.0, 157.0, 163.0, 157.0, 159.0, 160.0, 157.0, 160.0, 160.0, 160.0, 160.0, 160.0, 164.0, 159.0, 169.0, 169.0, 168.0, 163.0, 163.0, 163.0, 160.0, 170.0, 164.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 159.0, 161.0, 165.0, 167.0, 165.0, 167.0, 165.0, 167.0, 165.0, 167.0, 165.0, 160.0, 165.0, 160.0, 165.0, 155.0, 168.0, 165.0, 155.0, 155.0, 165.0, 155.0, 156.0, 156.0, 156.0, 155.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 164.0, 160.0, 160.0, 160.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 160.0, 160.0, 164.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 164.0, 164.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 164.0, 164.0, 164.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 157.0, 162.0, 162.0, 157.0, 162.0, 157.0, 162.0, 162.0, 162.0, 157.0, 163.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 158.0, 159.0, 164.0, 157.0, 164.0, 155.0, 155.0, 155.0, 155.0, 155.0, 157.0, 157.0, 155.0, 155.0, 155.0, 155.0, 160.0, 164.0, 164.0, 162.0, 164.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 160.0]\n",
            "Best num_ones with epsilon greedy=0.30000000000000004 is 151.0\n",
            "[160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 157.0, 157.0, 164.0, 164.0, 159.0, 159.0, 159.0, 159.0, 159.0, 164.0, 159.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 164.0, 159.0, 159.0, 173.0, 173.0, 173.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 153.0, 156.0, 164.0, 156.0, 157.0, 166.0, 157.0, 157.0, 157.0, 166.0, 166.0, 165.0, 165.0, 165.0, 160.0, 159.0, 161.0, 161.0, 161.0, 163.0, 161.0, 161.0, 163.0, 161.0, 163.0, 161.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 162.0, 162.0, 162.0, 158.0, 158.0, 160.0, 158.0, 158.0, 160.0, 160.0, 160.0, 158.0, 158.0, 158.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 154.0, 159.0, 159.0, 159.0, 159.0, 159.0, 154.0, 164.0, 164.0, 162.0, 157.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 155.0, 155.0, 168.0, 168.0, 155.0, 155.0, 155.0, 168.0, 168.0, 155.0, 155.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 159.0, 159.0, 159.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 154.0, 173.0, 173.0, 154.0, 173.0, 173.0, 173.0, 173.0, 168.0, 168.0, 164.0, 165.0, 165.0, 165.0, 165.0, 165.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 163.0, 165.0, 163.0, 163.0, 163.0, 165.0, 163.0, 163.0, 163.0, 163.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 159.0, 159.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 153.0, 156.0, 160.0, 160.0, 153.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 157.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 162.0, 162.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 155.0, 159.0, 155.0, 159.0, 155.0, 155.0, 155.0, 171.0, 171.0, 171.0, 171.0, 152.0, 152.0, 152.0, 152.0, 147.0, 147.0, 147.0, 147.0, 164.0, 147.0, 147.0, 147.0, 147.0, 164.0, 147.0, 165.0, 164.0, 166.0, 163.0, 147.0, 147.0, 147.0, 147.0, 147.0, 147.0, 147.0, 147.0, 147.0, 147.0, 150.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 164.0, 168.0, 155.0, 168.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 163.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 161.0, 161.0, 171.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 164.0, 165.0, 165.0, 165.0, 165.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 160.0, 162.0, 162.0, 162.0, 162.0, 164.0, 164.0, 164.0, 161.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 161.0, 161.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 160.0, 168.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 159.0, 159.0, 163.0, 159.0, 163.0, 163.0, 159.0, 162.0, 162.0, 170.0, 170.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 162.0, 162.0, 162.0, 161.0, 162.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 162.0, 158.0, 161.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 162.0, 159.0, 165.0, 165.0, 163.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 162.0, 164.0, 165.0, 165.0, 158.0, 158.0, 158.0, 158.0, 158.0, 163.0, 163.0, 163.0, 163.0, 158.0, 163.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 151.0, 151.0, 151.0, 151.0, 151.0, 155.0, 159.0, 159.0, 164.0, 164.0, 164.0, 160.0, 165.0, 165.0, 165.0, 152.0, 152.0, 159.0, 159.0, 152.0, 152.0, 152.0, 152.0, 152.0, 154.0, 154.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 165.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 163.0, 158.0, 158.0, 165.0, 165.0, 160.0, 165.0, 153.0, 156.0, 153.0, 153.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 164.0, 156.0, 164.0, 156.0, 164.0, 156.0, 156.0, 164.0, 156.0, 156.0, 164.0, 156.0, 163.0, 163.0, 163.0, 163.0, 163.0, 165.0, 165.0, 163.0, 163.0, 163.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 168.0, 162.0, 165.0, 165.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 152.0, 153.0, 153.0, 153.0, 152.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 160.0, 155.0, 155.0, 155.0, 155.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 153.0, 161.0, 161.0, 160.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 157.0, 163.0, 169.0, 163.0, 163.0, 169.0, 163.0, 163.0, 169.0, 163.0, 163.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 151.0, 151.0, 151.0, 151.0, 151.0, 155.0, 159.0, 155.0, 159.0, 159.0, 164.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 168.0, 168.0, 168.0, 155.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 158.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 166.0, 173.0, 173.0, 173.0, 173.0, 154.0, 154.0, 173.0, 154.0, 151.0, 151.0, 162.0, 162.0, 151.0, 151.0, 151.0, 162.0, 162.0, 162.0, 162.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 155.0, 155.0, 155.0, 155.0, 159.0, 155.0, 159.0, 159.0, 155.0, 159.0, 159.0, 150.0, 150.0, 150.0, 150.0, 150.0, 160.0, 155.0, 158.0, 158.0, 160.0, 166.0, 160.0, 165.0, 165.0, 165.0, 171.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 169.0, 158.0, 158.0, 158.0, 158.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 158.0, 165.0, 158.0, 158.0, 165.0, 165.0, 165.0, 158.0, 158.0, 165.0, 166.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 163.0, 165.0, 163.0, 165.0, 163.0, 163.0, 168.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 164.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 163.0, 163.0, 163.0, 163.0, 163.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 155.0, 155.0, 155.0, 155.0, 159.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 151.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 172.0, 161.0, 172.0, 161.0, 160.0, 160.0, 160.0, 158.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 154.0, 164.0, 164.0, 164.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 155.0, 155.0, 155.0, 159.0, 155.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 157.0, 157.0, 166.0, 157.0, 166.0, 157.0, 157.0, 166.0, 157.0, 157.0, 157.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 153.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 153.0, 156.0, 156.0, 156.0, 156.0, 152.0, 153.0, 152.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 165.0, 160.0, 165.0, 173.0, 173.0, 173.0, 173.0, 154.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 158.0, 159.0, 164.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0]\n",
            "Best num_ones with epsilon greedy=0.4 is 147.0\n",
            "[160.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 163.0, 163.0, 160.0, 160.0, 163.0, 166.0, 163.0, 166.0, 163.0, 166.0, 163.0, 163.0, 166.0, 150.0, 166.0, 156.0, 150.0, 156.0, 150.0, 150.0, 150.0, 159.0, 159.0, 159.0, 165.0, 155.0, 155.0, 155.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 162.0, 162.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 153.0, 153.0, 153.0, 164.0, 157.0, 157.0, 157.0, 160.0, 157.0, 157.0, 157.0, 160.0, 154.0, 165.0, 161.0, 158.0, 158.0, 158.0, 166.0, 157.0, 157.0, 157.0, 157.0, 165.0, 155.0, 168.0, 155.0, 155.0, 162.0, 162.0, 160.0, 162.0, 160.0, 162.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 162.0, 159.0, 159.0, 159.0, 162.0, 159.0, 159.0, 159.0, 159.0, 165.0, 152.0, 152.0, 152.0, 159.0, 152.0, 152.0, 152.0, 159.0, 152.0, 152.0, 159.0, 165.0, 160.0, 165.0, 165.0, 165.0, 159.0, 165.0, 159.0, 159.0, 159.0, 159.0, 165.0, 159.0, 165.0, 159.0, 159.0, 165.0, 155.0, 160.0, 159.0, 160.0, 159.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 159.0, 159.0, 151.0, 155.0, 159.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 165.0, 165.0, 165.0, 153.0, 153.0, 153.0, 167.0, 153.0, 167.0, 167.0, 153.0, 153.0, 156.0, 158.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 155.0, 168.0, 168.0, 155.0, 155.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 164.0, 162.0, 162.0, 162.0, 162.0, 165.0, 165.0, 160.0, 165.0, 153.0, 153.0, 153.0, 156.0, 156.0, 153.0, 153.0, 153.0, 153.0, 153.0, 156.0, 165.0, 163.0, 163.0, 169.0, 163.0, 163.0, 169.0, 163.0, 163.0, 163.0, 163.0, 163.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 171.0, 164.0, 171.0, 171.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 164.0, 161.0, 156.0, 161.0, 161.0, 156.0, 161.0, 161.0, 156.0, 161.0, 161.0, 161.0, 156.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 164.0, 159.0, 164.0, 164.0, 163.0, 155.0, 157.0, 157.0, 163.0, 163.0, 168.0, 172.0, 172.0, 172.0, 168.0, 166.0, 165.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 167.0, 165.0, 167.0, 167.0, 167.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 154.0, 173.0, 173.0, 173.0, 154.0, 173.0, 173.0, 173.0, 173.0, 173.0, 154.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 158.0, 158.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 151.0, 154.0, 154.0, 165.0, 165.0, 151.0, 151.0, 151.0, 151.0, 155.0, 155.0, 168.0, 166.0, 168.0, 158.0, 162.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 153.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 164.0, 164.0, 156.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 158.0, 158.0, 158.0, 159.0, 159.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 165.0, 160.0, 159.0, 152.0, 167.0, 152.0, 152.0, 152.0, 152.0, 152.0, 152.0, 152.0, 167.0, 152.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 171.0, 171.0, 171.0, 171.0, 171.0, 171.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 162.0, 162.0, 163.0, 163.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 159.0, 152.0, 152.0, 152.0, 152.0, 152.0, 167.0, 152.0, 152.0, 167.0, 152.0, 157.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 158.0, 163.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 163.0, 165.0, 165.0, 159.0, 159.0, 159.0, 155.0, 155.0, 164.0, 166.0, 166.0, 164.0, 166.0, 164.0, 164.0, 166.0, 166.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 158.0, 163.0, 163.0, 163.0, 158.0, 163.0, 158.0, 158.0, 158.0, 158.0, 158.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 171.0, 171.0, 170.0, 165.0, 173.0, 173.0, 173.0, 165.0, 160.0, 165.0, 165.0, 165.0, 151.0, 155.0, 159.0, 159.0, 159.0, 159.0, 155.0, 155.0, 159.0, 163.0, 172.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 153.0, 153.0, 153.0, 157.0, 169.0, 169.0, 157.0, 157.0, 169.0, 163.0, 163.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 162.0, 162.0, 164.0, 162.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 173.0, 163.0, 163.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 157.0, 152.0, 166.0, 152.0, 166.0, 166.0, 152.0, 152.0, 152.0, 152.0, 152.0, 152.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 150.0, 156.0, 150.0, 150.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 161.0, 171.0, 171.0, 171.0, 171.0, 171.0, 161.0, 171.0, 171.0, 171.0, 160.0, 164.0, 171.0, 164.0, 171.0, 171.0, 169.0, 167.0, 169.0, 169.0, 169.0, 160.0, 165.0, 165.0, 165.0, 165.0, 150.0, 150.0, 150.0, 150.0, 156.0, 150.0, 150.0, 157.0, 157.0, 157.0, 157.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 173.0, 160.0, 165.0, 165.0, 165.0, 152.0, 152.0, 152.0, 159.0, 152.0, 159.0, 152.0, 159.0, 159.0, 159.0, 152.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 159.0, 160.0, 160.0, 160.0, 152.0, 152.0, 160.0, 160.0, 160.0, 152.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 163.0, 165.0, 165.0, 165.0, 163.0, 159.0, 154.0, 154.0, 166.0, 162.0, 159.0, 159.0, 159.0, 158.0, 158.0, 158.0, 159.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 165.0, 163.0, 157.0, 159.0, 157.0, 159.0, 164.0, 164.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 158.0, 158.0, 158.0, 158.0, 159.0, 164.0, 159.0, 164.0, 164.0, 164.0, 159.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 157.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 169.0, 169.0, 163.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 164.0, 169.0, 159.0, 169.0, 169.0, 159.0, 157.0, 157.0, 167.0, 167.0, 167.0, 167.0, 157.0, 167.0, 167.0, 168.0, 168.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 165.0, 165.0, 157.0, 157.0, 164.0, 159.0, 164.0, 159.0, 164.0, 159.0, 159.0, 163.0, 162.0, 165.0, 159.0, 161.0, 161.0, 158.0, 165.0, 158.0, 158.0, 165.0, 158.0, 158.0, 163.0, 163.0, 160.0, 163.0, 153.0, 153.0, 153.0, 153.0, 153.0, 167.0, 153.0, 153.0, 159.0, 165.0, 156.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 158.0, 158.0, 158.0, 158.0, 159.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 167.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 165.0, 159.0, 165.0, 159.0, 159.0, 163.0, 163.0, 163.0, 163.0, 165.0, 165.0, 165.0, 164.0, 165.0, 159.0, 156.0, 156.0, 156.0, 156.0, 170.0, 170.0, 170.0, 170.0, 170.0, 170.0, 170.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 159.0, 162.0, 159.0, 162.0, 159.0, 162.0, 159.0, 163.0, 163.0, 163.0, 157.0, 157.0, 163.0, 163.0, 163.0, 168.0, 168.0, 164.0, 163.0, 164.0, 164.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 152.0, 159.0, 152.0, 159.0, 152.0, 152.0, 152.0, 152.0, 152.0, 152.0, 156.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 162.0, 162.0, 162.0, 162.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 165.0, 165.0, 158.0, 152.0, 152.0, 165.0, 152.0, 152.0, 152.0, 152.0, 152.0, 152.0, 152.0, 152.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 153.0, 152.0, 152.0, 167.0, 152.0, 152.0, 156.0, 163.0, 154.0, 154.0, 154.0, 154.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 164.0, 164.0, 164.0, 164.0, 164.0, 160.0, 155.0, 155.0, 160.0, 150.0, 162.0, 159.0, 164.0, 164.0, 164.0, 160.0, 169.0, 163.0, 169.0, 163.0, 160.0, 164.0, 171.0, 164.0, 171.0, 171.0, 171.0, 171.0, 171.0, 171.0, 171.0, 165.0, 164.0, 164.0, 159.0, 159.0, 164.0, 159.0, 164.0, 164.0, 164.0, 159.0, 163.0, 163.0, 163.0, 165.0, 165.0, 165.0, 165.0, 153.0, 153.0, 153.0, 153.0, 153.0, 167.0, 168.0, 168.0, 168.0, 168.0, 168.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 157.0, 163.0, 163.0, 163.0, 163.0, 163.0, 165.0, 165.0, 164.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 155.0, 155.0, 160.0, 160.0, 160.0, 159.0, 160.0, 160.0, 160.0, 159.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 159.0, 164.0, 164.0, 164.0, 164.0, 159.0, 159.0, 164.0, 164.0, 170.0, 164.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 154.0, 154.0, 155.0, 154.0, 154.0, 154.0, 155.0, 154.0, 155.0, 154.0, 154.0, 160.0]\n",
            "Best num_ones with epsilon greedy=0.5 is 150.0\n",
            "[165.0, 165.0, 165.0, 165.0, 165.0, 155.0, 160.0, 160.0, 162.0, 161.0, 161.0, 161.0, 161.0, 169.0, 164.0, 166.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 153.0, 153.0, 153.0, 153.0, 156.0, 163.0, 162.0, 153.0, 156.0, 161.0, 161.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 156.0, 156.0, 156.0, 156.0, 164.0, 156.0, 156.0, 164.0, 156.0, 156.0, 156.0, 165.0, 165.0, 157.0, 157.0, 162.0, 162.0, 162.0, 162.0, 162.0, 157.0, 157.0, 159.0, 157.0, 165.0, 165.0, 159.0, 159.0, 154.0, 160.0, 160.0, 160.0, 163.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 165.0, 160.0, 159.0, 154.0, 156.0, 156.0, 165.0, 165.0, 156.0, 165.0, 156.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 151.0, 159.0, 159.0, 161.0, 159.0, 157.0, 168.0, 166.0, 166.0, 166.0, 166.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 159.0, 165.0, 165.0, 165.0, 160.0, 159.0, 154.0, 154.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 162.0, 162.0, 165.0, 160.0, 160.0, 165.0, 160.0, 159.0, 159.0, 159.0, 165.0, 159.0, 160.0, 160.0, 160.0, 159.0, 165.0, 159.0, 165.0, 165.0, 157.0, 162.0, 152.0, 162.0, 158.0, 159.0, 159.0, 165.0, 165.0, 165.0, 157.0, 158.0, 158.0, 160.0, 160.0, 158.0, 154.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 165.0, 160.0, 158.0, 158.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 159.0, 161.0, 159.0, 165.0, 162.0, 158.0, 162.0, 162.0, 158.0, 158.0, 162.0, 158.0, 162.0, 158.0, 168.0, 168.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 168.0, 168.0, 168.0, 168.0, 168.0, 160.0, 160.0, 160.0, 166.0, 160.0, 165.0, 165.0, 160.0, 165.0, 158.0, 158.0, 163.0, 159.0, 156.0, 158.0, 158.0, 158.0, 158.0, 158.0, 164.0, 161.0, 164.0, 164.0, 164.0, 165.0, 165.0, 165.0, 165.0, 160.0, 159.0, 159.0, 152.0, 152.0, 152.0, 152.0, 167.0, 167.0, 152.0, 167.0, 167.0, 167.0, 152.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 171.0, 164.0, 164.0, 160.0, 161.0, 161.0, 161.0, 158.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 159.0, 159.0, 159.0, 154.0, 155.0, 154.0, 156.0, 165.0, 165.0, 165.0, 156.0, 165.0, 165.0, 165.0, 165.0, 165.0, 163.0, 163.0, 163.0, 165.0, 163.0, 159.0, 154.0, 160.0, 163.0, 160.0, 172.0, 161.0, 161.0, 163.0, 162.0, 165.0, 159.0, 165.0, 165.0, 160.0, 159.0, 159.0, 154.0, 164.0, 164.0, 164.0, 164.0, 154.0, 164.0, 164.0, 157.0, 151.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 151.0, 151.0, 157.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 159.0, 154.0, 154.0, 156.0, 165.0, 156.0, 165.0, 165.0, 162.0, 160.0, 160.0, 160.0, 159.0, 159.0, 157.0, 156.0, 159.0, 156.0, 159.0, 156.0, 156.0, 156.0, 159.0, 159.0, 156.0, 156.0, 160.0, 160.0, 160.0, 159.0, 154.0, 156.0, 165.0, 165.0, 165.0, 165.0, 165.0, 156.0, 165.0, 165.0, 156.0, 160.0, 159.0, 165.0, 159.0, 159.0, 159.0, 154.0, 156.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 159.0, 161.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 166.0, 163.0, 165.0, 160.0, 159.0, 159.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 165.0, 160.0, 165.0, 165.0, 165.0, 162.0, 164.0, 164.0, 162.0, 162.0, 164.0, 154.0, 165.0, 153.0, 165.0, 153.0, 165.0, 165.0, 165.0, 153.0, 165.0, 165.0, 161.0, 162.0, 165.0, 171.0, 171.0, 171.0, 164.0, 164.0, 160.0, 162.0, 161.0, 162.0, 161.0, 171.0, 163.0, 162.0, 164.0, 164.0, 164.0, 165.0, 165.0, 165.0, 160.0, 159.0, 158.0, 159.0, 164.0, 164.0, 164.0, 153.0, 153.0, 154.0, 160.0, 163.0, 173.0, 165.0, 153.0, 165.0, 164.0, 154.0, 165.0, 160.0, 165.0, 165.0, 160.0, 159.0, 165.0, 159.0, 165.0, 159.0, 154.0, 159.0, 159.0, 154.0, 154.0, 156.0, 167.0, 167.0, 158.0, 161.0, 161.0, 165.0, 165.0, 165.0, 171.0, 171.0, 171.0, 171.0, 171.0, 161.0, 171.0, 171.0, 161.0, 161.0, 156.0, 165.0, 165.0, 165.0, 165.0, 165.0, 156.0, 165.0, 156.0, 165.0, 156.0, 160.0, 160.0, 159.0, 154.0, 165.0, 157.0, 157.0, 153.0, 165.0, 153.0, 165.0, 161.0, 155.0, 161.0, 155.0, 155.0, 155.0, 155.0, 165.0, 160.0, 159.0, 159.0, 165.0, 165.0, 159.0, 154.0, 159.0, 159.0, 159.0, 159.0, 159.0, 154.0, 154.0, 160.0, 165.0, 165.0, 165.0, 165.0, 159.0, 154.0, 156.0, 160.0, 164.0, 162.0, 166.0, 164.0, 164.0, 164.0, 164.0, 164.0, 160.0, 165.0, 160.0, 159.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 154.0, 164.0, 154.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 154.0, 165.0, 165.0, 160.0, 159.0, 159.0, 159.0, 154.0, 154.0, 160.0, 159.0, 154.0, 156.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 162.0, 162.0, 167.0, 167.0, 167.0, 167.0, 167.0, 157.0, 159.0, 159.0, 159.0, 157.0, 159.0, 165.0, 165.0, 156.0, 165.0, 165.0, 165.0, 165.0, 156.0, 156.0, 165.0, 156.0, 151.0, 156.0, 156.0, 160.0, 156.0, 156.0, 156.0, 156.0, 170.0, 170.0, 160.0, 160.0, 157.0, 157.0, 157.0, 157.0, 157.0, 164.0, 166.0, 155.0, 163.0, 165.0, 165.0, 164.0, 162.0, 162.0, 164.0, 162.0, 166.0, 155.0, 155.0, 155.0, 155.0, 158.0, 156.0, 151.0, 160.0, 151.0, 151.0, 151.0, 151.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 153.0, 167.0, 167.0, 166.0, 166.0, 166.0, 166.0, 166.0, 160.0, 160.0, 160.0, 165.0, 165.0, 155.0, 159.0, 159.0, 159.0, 155.0, 159.0, 154.0, 156.0, 156.0, 154.0, 154.0, 156.0, 156.0, 156.0, 166.0, 166.0, 161.0, 165.0, 165.0, 153.0, 167.0, 166.0, 166.0, 163.0, 163.0, 167.0, 168.0, 168.0, 162.0, 162.0, 159.0, 159.0, 154.0, 154.0, 173.0, 154.0, 160.0, 157.0, 156.0, 156.0, 156.0, 156.0, 156.0, 160.0, 159.0, 159.0, 159.0, 154.0, 154.0, 154.0, 156.0, 165.0, 165.0, 165.0, 163.0, 165.0, 157.0, 156.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 158.0, 163.0, 161.0, 165.0, 165.0, 165.0, 153.0, 153.0, 164.0, 158.0, 153.0, 153.0, 156.0, 161.0, 161.0, 156.0, 153.0, 159.0, 164.0, 159.0, 159.0, 159.0, 159.0, 159.0, 162.0, 160.0, 160.0, 160.0, 160.0, 159.0, 154.0, 165.0, 153.0, 153.0, 165.0, 165.0, 164.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 165.0, 160.0, 160.0, 159.0, 154.0, 156.0, 165.0, 165.0, 165.0, 166.0, 166.0, 166.0, 166.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 162.0, 162.0, 162.0, 164.0, 164.0, 161.0, 161.0, 155.0, 161.0, 161.0, 155.0, 155.0, 155.0, 155.0, 161.0, 161.0, 155.0, 155.0, 164.0, 160.0, 164.0, 164.0, 160.0, 160.0, 162.0, 162.0, 161.0, 161.0, 162.0, 161.0, 165.0, 165.0, 165.0, 165.0, 165.0, 155.0, 155.0, 155.0, 155.0, 160.0, 161.0, 161.0, 161.0, 156.0, 162.0, 162.0, 171.0, 161.0, 161.0, 157.0, 163.0, 163.0, 164.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 165.0, 160.0, 159.0, 154.0, 159.0, 154.0, 159.0, 159.0, 154.0, 154.0, 154.0, 159.0, 159.0, 154.0, 156.0, 156.0, 156.0, 160.0, 163.0, 164.0, 164.0, 164.0, 171.0, 171.0, 161.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 159.0, 165.0, 159.0, 165.0, 165.0, 159.0, 154.0, 154.0, 159.0, 154.0, 154.0, 159.0, 159.0, 154.0, 156.0, 165.0, 165.0, 165.0, 160.0, 159.0, 165.0, 165.0, 161.0, 161.0, 161.0, 166.0, 166.0, 167.0, 167.0, 166.0, 160.0, 165.0, 165.0, 160.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 159.0, 165.0, 159.0, 160.0, 151.0, 151.0, 155.0, 159.0, 159.0, 155.0, 155.0, 155.0, 155.0, 159.0, 159.0, 165.0, 165.0, 165.0, 160.0, 159.0, 154.0, 155.0, 165.0, 157.0, 156.0, 156.0, 156.0, 168.0, 172.0, 161.0, 161.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 159.0, 154.0, 154.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 160.0, 159.0, 154.0, 154.0, 154.0, 154.0, 156.0, 165.0, 165.0, 156.0, 172.0, 161.0, 172.0, 165.0, 165.0, 165.0, 153.0, 153.0, 153.0, 156.0, 167.0, 167.0, 163.0, 158.0, 158.0, 158.0, 163.0, 165.0, 165.0, 165.0, 160.0, 159.0, 159.0, 164.0, 164.0, 164.0, 160.0, 160.0, 160.0, 160.0, 162.0, 161.0, 159.0, 159.0, 159.0, 159.0, 159.0, 164.0, 169.0, 169.0, 163.0, 163.0, 163.0, 165.0, 165.0, 157.0, 162.0, 157.0, 162.0, 157.0, 157.0, 162.0, 162.0, 162.0, 157.0, 162.0, 165.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 150.0, 159.0, 162.0, 162.0, 161.0, 158.0, 158.0, 158.0, 161.0, 161.0, 162.0, 165.0, 160.0, 159.0, 154.0, 154.0, 156.0, 156.0, 165.0, 165.0, 156.0, 165.0, 165.0, 156.0, 156.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 160.0, 154.0, 154.0, 160.0, 160.0, 157.0, 156.0, 156.0, 171.0, 171.0, 163.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 159.0, 161.0, 164.0, 164.0, 164.0, 161.0, 159.0, 154.0, 154.0, 156.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 156.0, 165.0, 165.0, 165.0, 160.0, 159.0, 165.0, 161.0, 160.0, 158.0, 158.0, 161.0, 154.0, 154.0, 154.0, 154.0, 154.0, 161.0, 154.0, 154.0, 154.0, 161.0, 154.0, 165.0, 157.0, 161.0, 161.0, 161.0, 158.0, 161.0, 162.0, 170.0, 170.0, 170.0, 170.0, 165.0, 160.0, 151.0, 151.0, 159.0, 157.0, 157.0, 168.0, 158.0, 158.0, 158.0, 158.0, 158.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 159.0, 154.0, 159.0, 162.0, 164.0, 154.0, 156.0, 156.0, 165.0, 165.0, 165.0, 156.0, 165.0, 165.0, 165.0, 160.0, 165.0, 153.0, 156.0, 159.0, 159.0, 159.0, 159.0, 159.0, 162.0, 159.0, 159.0, 159.0, 160.0, 159.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 154.0, 156.0, 156.0, 165.0, 151.0, 166.0, 151.0, 151.0, 166.0, 151.0, 151.0, 166.0, 166.0, 166.0, 166.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 159.0, 159.0, 159.0, 159.0, 154.0, 156.0, 165.0, 156.0, 165.0, 156.0, 156.0, 156.0, 169.0, 170.0, 169.0, 165.0, 165.0, 160.0, 159.0, 159.0, 154.0, 156.0, 165.0, 165.0, 155.0, 157.0, 155.0, 160.0, 162.0, 162.0, 162.0, 165.0, 165.0, 158.0, 158.0, 163.0, 168.0, 173.0, 165.0, 155.0, 155.0, 151.0, 158.0, 158.0, 158.0, 167.0, 167.0, 167.0, 167.0, 167.0, 167.0, 166.0, 160.0, 159.0, 154.0, 156.0, 162.0, 154.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 165.0, 160.0, 159.0, 164.0, 153.0, 165.0, 153.0, 165.0, 165.0, 161.0, 155.0, 155.0, 161.0, 161.0, 161.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 159.0, 165.0, 159.0, 154.0, 154.0, 154.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 154.0, 160.0, 163.0, 173.0, 154.0, 160.0, 163.0, 163.0, 173.0, 154.0, 154.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 158.0, 159.0, 159.0, 159.0, 164.0, 168.0, 168.0, 173.0, 173.0, 173.0, 157.0, 154.0, 154.0, 154.0, 154.0, 163.0, 158.0, 158.0, 163.0, 163.0, 153.0, 163.0, 153.0, 153.0, 163.0, 155.0, 158.0, 159.0, 158.0, 160.0, 170.0, 165.0, 165.0, 160.0, 159.0, 154.0, 159.0, 159.0, 159.0, 154.0, 156.0, 165.0, 156.0, 165.0, 165.0, 165.0, 153.0, 152.0, 152.0, 152.0, 152.0, 152.0, 152.0, 152.0, 153.0, 152.0, 152.0, 153.0, 160.0, 160.0, 159.0, 154.0, 154.0, 154.0, 154.0, 159.0, 159.0, 159.0, 154.0, 159.0, 159.0, 159.0, 165.0, 165.0, 165.0, 160.0, 159.0, 154.0, 154.0, 156.0, 156.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 166.0, 166.0, 163.0, 166.0, 159.0, 163.0, 169.0, 172.0, 162.0, 153.0, 153.0, 153.0, 156.0, 156.0, 160.0, 161.0, 160.0, 160.0, 161.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 154.0, 154.0, 156.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 156.0, 165.0, 165.0, 160.0, 159.0, 164.0, 160.0, 160.0, 162.0, 161.0, 161.0, 162.0, 162.0, 162.0, 161.0, 165.0, 160.0, 165.0, 160.0, 159.0, 154.0, 156.0, 155.0, 169.0, 168.0, 164.0, 163.0, 163.0, 165.0, 158.0, 158.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 159.0, 154.0, 159.0, 154.0, 156.0, 165.0, 165.0, 165.0, 165.0, 155.0, 163.0, 155.0, 160.0, 164.0, 161.0, 161.0, 161.0, 154.0, 154.0, 160.0, 160.0, 161.0, 160.0, 161.0, 161.0, 159.0, 161.0, 162.0, 165.0, 165.0, 163.0, 163.0, 165.0, 164.0, 163.0, 160.0, 157.0, 157.0, 156.0, 156.0, 157.0, 156.0, 157.0, 156.0, 165.0, 173.0, 154.0, 173.0, 166.0, 159.0, 159.0, 166.0, 166.0, 159.0, 169.0, 164.0, 164.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 151.0, 151.0, 157.0, 162.0, 162.0, 157.0, 154.0, 155.0, 154.0, 154.0, 166.0, 165.0, 165.0, 150.0, 155.0, 155.0, 155.0, 155.0, 155.0, 158.0, 155.0, 155.0, 155.0, 155.0, 165.0]\n",
            "Best num_ones with epsilon greedy=0.6000000000000001 is 150.0\n",
            "[160.0, 160.0, 153.0, 153.0, 159.0, 159.0, 156.0, 164.0, 153.0, 153.0, 156.0, 164.0, 164.0, 165.0, 165.0, 159.0, 164.0, 164.0, 154.0, 164.0, 164.0, 159.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 164.0, 160.0, 153.0, 159.0, 159.0, 156.0, 164.0, 164.0, 153.0, 159.0, 159.0, 156.0, 164.0, 157.0, 167.0, 167.0, 157.0, 163.0, 166.0, 164.0, 164.0, 164.0, 164.0, 166.0, 165.0, 159.0, 164.0, 159.0, 164.0, 164.0, 164.0, 164.0, 167.0, 167.0, 163.0, 166.0, 165.0, 159.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 159.0, 164.0, 159.0, 164.0, 164.0, 159.0, 164.0, 157.0, 157.0, 164.0, 164.0, 157.0, 157.0, 164.0, 164.0, 164.0, 161.0, 153.0, 159.0, 159.0, 161.0, 159.0, 161.0, 159.0, 161.0, 161.0, 159.0, 159.0, 165.0, 165.0, 160.0, 153.0, 153.0, 163.0, 168.0, 168.0, 165.0, 173.0, 173.0, 173.0, 168.0, 159.0, 165.0, 152.0, 152.0, 159.0, 152.0, 159.0, 159.0, 152.0, 150.0, 155.0, 147.0, 147.0, 162.0, 155.0, 157.0, 147.0, 162.0, 170.0, 154.0, 154.0, 150.0, 159.0, 164.0, 159.0, 164.0, 167.0, 167.0, 157.0, 157.0, 167.0, 157.0, 157.0, 157.0, 164.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 159.0, 159.0, 164.0, 159.0, 159.0, 164.0, 159.0, 164.0, 164.0, 165.0, 165.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 153.0, 159.0, 168.0, 168.0, 159.0, 162.0, 159.0, 159.0, 159.0, 169.0, 169.0, 160.0, 153.0, 159.0, 159.0, 164.0, 164.0, 159.0, 162.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 164.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 159.0, 158.0, 158.0, 167.0, 167.0, 158.0, 167.0, 167.0, 167.0, 167.0, 167.0, 158.0, 165.0, 160.0, 153.0, 159.0, 156.0, 164.0, 169.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 165.0, 159.0, 163.0, 163.0, 163.0, 155.0, 155.0, 160.0, 160.0, 160.0, 162.0, 159.0, 165.0, 170.0, 170.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 164.0, 165.0, 165.0, 159.0, 159.0, 164.0, 160.0, 165.0, 165.0, 159.0, 164.0, 164.0, 165.0, 159.0, 164.0, 164.0, 156.0, 164.0, 156.0, 164.0, 157.0, 164.0, 165.0, 159.0, 159.0, 167.0, 159.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 162.0, 155.0, 160.0, 164.0, 164.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 162.0, 160.0, 164.0, 164.0, 165.0, 157.0, 168.0, 156.0, 159.0, 154.0, 160.0, 157.0, 164.0, 152.0, 152.0, 162.0, 162.0, 162.0, 162.0, 164.0, 164.0, 165.0, 157.0, 157.0, 159.0, 159.0, 159.0, 164.0, 165.0, 159.0, 164.0, 164.0, 164.0, 159.0, 159.0, 160.0, 160.0, 153.0, 156.0, 156.0, 164.0, 164.0, 164.0, 157.0, 157.0, 157.0, 157.0, 154.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 167.0, 167.0, 159.0, 164.0, 161.0, 162.0, 164.0, 164.0, 165.0, 165.0, 165.0, 162.0, 162.0, 165.0, 159.0, 159.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 159.0, 159.0, 159.0, 160.0, 159.0, 159.0, 159.0, 162.0, 152.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 163.0, 163.0, 163.0, 163.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 159.0, 162.0, 162.0, 168.0, 168.0, 160.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 164.0, 164.0, 167.0, 169.0, 169.0, 169.0, 169.0, 159.0, 159.0, 164.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 165.0, 160.0, 153.0, 160.0, 160.0, 161.0, 160.0, 160.0, 160.0, 160.0, 161.0, 161.0, 161.0, 159.0, 159.0, 159.0, 163.0, 156.0, 156.0, 170.0, 170.0, 161.0, 161.0, 170.0, 170.0, 165.0, 166.0, 165.0, 165.0, 159.0, 164.0, 166.0, 157.0, 157.0, 159.0, 151.0, 159.0, 158.0, 158.0, 161.0, 161.0, 161.0, 165.0, 165.0, 165.0, 165.0, 160.0, 153.0, 153.0, 159.0, 156.0, 164.0, 164.0, 164.0, 161.0, 168.0, 168.0, 164.0, 165.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 164.0, 165.0, 159.0, 159.0, 164.0, 164.0, 164.0, 159.0, 164.0, 159.0, 164.0, 164.0, 159.0, 160.0, 160.0, 166.0, 166.0, 166.0, 153.0, 153.0, 162.0, 162.0, 168.0, 158.0, 167.0, 154.0, 166.0, 167.0, 154.0, 160.0, 160.0, 160.0, 153.0, 161.0, 153.0, 156.0, 164.0, 153.0, 159.0, 159.0, 153.0, 156.0, 156.0, 165.0, 159.0, 164.0, 164.0, 166.0, 160.0, 160.0, 166.0, 166.0, 160.0, 160.0, 166.0, 160.0, 160.0, 153.0, 164.0, 156.0, 156.0, 156.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 164.0, 165.0, 159.0, 164.0, 159.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 159.0, 162.0, 161.0, 162.0, 165.0, 165.0, 160.0, 153.0, 156.0, 159.0, 169.0, 169.0, 169.0, 169.0, 169.0, 169.0, 169.0, 159.0, 165.0, 159.0, 164.0, 164.0, 164.0, 159.0, 159.0, 164.0, 159.0, 164.0, 159.0, 159.0, 160.0, 165.0, 168.0, 159.0, 167.0, 159.0, 167.0, 167.0, 154.0, 167.0, 154.0, 167.0, 154.0, 167.0, 158.0, 158.0, 161.0, 157.0, 164.0, 160.0, 165.0, 165.0, 165.0, 160.0, 154.0, 163.0, 163.0, 163.0, 163.0, 158.0, 159.0, 162.0, 162.0, 161.0, 161.0, 159.0, 165.0, 157.0, 157.0, 168.0, 168.0, 168.0, 169.0, 169.0, 169.0, 169.0, 163.0, 169.0, 160.0, 165.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 153.0, 156.0, 153.0, 153.0, 153.0, 156.0, 153.0, 153.0, 156.0, 156.0, 153.0, 160.0, 153.0, 153.0, 161.0, 155.0, 166.0, 165.0, 165.0, 165.0, 165.0, 166.0, 166.0, 160.0, 165.0, 159.0, 164.0, 167.0, 167.0, 167.0, 167.0, 167.0, 167.0, 167.0, 167.0, 167.0, 159.0, 164.0, 164.0, 164.0, 159.0, 159.0, 164.0, 159.0, 161.0, 162.0, 161.0, 160.0, 160.0, 165.0, 159.0, 159.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 165.0, 160.0, 153.0, 153.0, 160.0, 168.0, 162.0, 162.0, 162.0, 162.0, 158.0, 165.0, 168.0, 165.0, 160.0, 153.0, 156.0, 164.0, 156.0, 164.0, 162.0, 155.0, 153.0, 154.0, 160.0, 155.0, 160.0, 165.0, 160.0, 160.0, 153.0, 159.0, 156.0, 164.0, 157.0, 157.0, 157.0, 157.0, 154.0, 154.0, 154.0, 160.0, 153.0, 159.0, 153.0, 153.0, 159.0, 159.0, 156.0, 164.0, 153.0, 153.0, 162.0, 165.0, 160.0, 165.0, 159.0, 159.0, 164.0, 155.0, 159.0, 155.0, 164.0, 164.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 155.0, 155.0, 164.0, 165.0, 160.0, 160.0, 158.0, 158.0, 164.0, 160.0, 160.0, 159.0, 164.0, 164.0, 164.0, 159.0, 164.0, 159.0, 164.0, 159.0, 159.0, 159.0, 160.0, 153.0, 159.0, 156.0, 164.0, 164.0, 164.0, 164.0, 155.0, 161.0, 161.0, 161.0, 165.0, 159.0, 164.0, 159.0, 159.0, 164.0, 159.0, 164.0, 159.0, 156.0, 156.0, 156.0, 156.0, 161.0, 152.0, 160.0, 160.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 159.0, 159.0, 159.0, 159.0, 159.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 159.0, 164.0, 164.0, 164.0, 159.0, 167.0, 167.0, 157.0, 164.0, 164.0, 158.0, 158.0, 158.0, 158.0, 167.0, 167.0, 167.0, 167.0, 159.0, 159.0, 164.0, 162.0, 162.0, 164.0, 150.0, 150.0, 154.0, 154.0, 154.0, 157.0, 157.0, 163.0, 168.0, 168.0, 163.0, 160.0, 163.0, 163.0, 163.0, 152.0, 159.0, 152.0, 159.0, 159.0, 152.0, 152.0, 159.0, 159.0, 159.0, 159.0, 159.0, 162.0, 162.0, 156.0, 156.0, 156.0, 156.0, 172.0, 172.0, 172.0, 158.0, 172.0, 158.0, 158.0, 159.0, 164.0, 163.0, 163.0, 163.0, 163.0, 171.0, 171.0, 163.0, 172.0, 172.0, 155.0, 168.0, 168.0, 155.0, 155.0, 168.0, 168.0, 155.0, 168.0, 155.0, 155.0, 160.0, 160.0, 165.0, 159.0, 159.0, 164.0, 162.0, 164.0, 159.0, 160.0, 162.0, 159.0, 162.0, 158.0, 158.0, 167.0, 160.0, 160.0, 148.0, 163.0, 163.0, 148.0, 161.0, 161.0, 164.0, 160.0, 159.0, 158.0, 162.0, 159.0, 159.0, 159.0, 159.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 151.0, 163.0, 151.0, 151.0, 158.0, 158.0, 158.0, 158.0, 158.0, 158.0, 168.0, 160.0, 160.0, 153.0, 150.0, 161.0, 158.0, 159.0, 159.0, 159.0, 159.0, 159.0, 165.0, 165.0, 165.0, 165.0, 160.0, 158.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 161.0, 165.0, 161.0, 160.0, 165.0, 160.0, 163.0, 152.0, 161.0, 161.0, 157.0, 161.0, 157.0, 157.0, 161.0, 161.0, 165.0, 154.0, 165.0, 165.0, 159.0, 164.0, 164.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 171.0, 171.0, 165.0, 159.0, 164.0, 164.0, 159.0, 164.0, 167.0, 156.0, 156.0, 158.0, 161.0, 161.0, 161.0, 161.0, 161.0, 165.0, 164.0, 164.0, 159.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 153.0, 161.0, 161.0, 156.0, 155.0, 163.0, 160.0, 162.0, 160.0, 163.0, 160.0, 157.0, 157.0, 162.0, 159.0, 159.0, 156.0, 156.0, 160.0, 158.0, 169.0, 169.0, 169.0, 169.0, 158.0, 158.0, 158.0, 159.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 164.0, 162.0, 165.0, 165.0, 165.0, 165.0, 173.0, 159.0, 165.0, 164.0, 173.0, 173.0, 164.0, 173.0, 164.0, 164.0, 154.0, 154.0, 160.0, 153.0, 154.0, 163.0, 160.0, 161.0, 161.0, 160.0, 161.0, 160.0, 160.0, 161.0, 160.0, 160.0, 160.0, 160.0, 160.0, 153.0, 159.0, 153.0, 159.0, 164.0, 156.0, 165.0, 165.0, 166.0, 165.0, 166.0, 159.0, 164.0, 154.0, 154.0, 173.0, 173.0, 173.0, 154.0, 154.0, 154.0, 173.0, 173.0, 164.0, 151.0, 162.0, 161.0, 150.0, 161.0, 161.0, 150.0, 150.0, 150.0, 155.0, 160.0, 160.0, 160.0, 160.0, 165.0, 159.0, 159.0, 158.0, 158.0, 158.0, 158.0, 158.0, 161.0, 161.0, 157.0, 153.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 155.0, 155.0, 159.0, 164.0, 161.0, 161.0, 161.0, 161.0, 155.0, 155.0, 155.0, 155.0, 155.0, 161.0, 155.0, 161.0, 161.0, 161.0, 167.0, 159.0, 159.0, 164.0, 164.0, 164.0, 164.0, 156.0, 156.0, 153.0, 153.0, 153.0, 156.0, 156.0, 153.0, 153.0, 153.0, 153.0, 153.0, 160.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 159.0, 164.0, 164.0, 164.0, 164.0, 165.0, 165.0, 160.0, 153.0, 153.0, 156.0, 156.0, 164.0, 164.0, 164.0, 161.0, 153.0, 159.0, 154.0, 165.0, 165.0, 157.0, 165.0, 167.0, 167.0, 167.0, 165.0, 167.0, 165.0, 167.0, 167.0, 165.0, 160.0, 153.0, 156.0, 154.0, 154.0, 163.0, 163.0, 159.0, 156.0, 164.0, 157.0, 157.0, 160.0, 153.0, 159.0, 153.0, 157.0, 154.0, 154.0, 157.0, 157.0, 157.0, 157.0, 157.0, 164.0, 164.0, 171.0, 157.0, 157.0, 161.0, 160.0, 171.0, 160.0, 161.0, 159.0, 162.0, 162.0, 164.0, 155.0, 161.0, 162.0, 162.0, 162.0, 161.0, 162.0, 162.0, 161.0, 162.0, 162.0, 165.0, 165.0, 165.0, 159.0, 165.0, 159.0, 165.0, 164.0, 164.0, 157.0, 157.0, 167.0, 167.0, 163.0, 163.0, 161.0, 161.0, 171.0, 163.0, 163.0, 159.0, 159.0, 159.0, 159.0, 164.0, 164.0, 164.0, 159.0, 159.0, 164.0, 164.0, 159.0, 151.0, 153.0, 156.0, 164.0, 164.0, 164.0, 157.0, 164.0, 164.0, 156.0, 156.0, 159.0, 164.0, 159.0, 164.0, 164.0, 164.0, 165.0, 160.0, 160.0, 157.0, 160.0, 157.0, 160.0, 160.0, 157.0, 160.0, 157.0, 157.0, 160.0, 157.0, 159.0, 159.0, 156.0, 164.0, 164.0, 156.0, 164.0, 158.0, 173.0, 164.0, 173.0, 173.0, 164.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 159.0, 159.0, 159.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 154.0, 154.0, 154.0, 154.0, 154.0, 173.0, 154.0, 154.0, 173.0, 154.0, 154.0, 159.0, 164.0, 159.0, 159.0, 164.0, 159.0, 164.0, 164.0, 164.0, 164.0, 159.0, 159.0]\n",
            "Best num_ones with epsilon greedy=0.7000000000000001 is 147.0\n",
            "[159.0, 159.0, 154.0, 154.0, 173.0, 173.0, 159.0, 156.0, 156.0, 159.0, 159.0, 154.0, 159.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 156.0, 164.0, 158.0, 158.0, 160.0, 160.0, 160.0, 160.0, 160.0, 158.0, 160.0, 165.0, 155.0, 155.0, 155.0, 168.0, 155.0, 168.0, 155.0, 155.0, 155.0, 168.0, 168.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 158.0, 158.0, 158.0, 166.0, 166.0, 166.0, 166.0, 168.0, 168.0, 166.0, 168.0, 165.0, 165.0, 160.0, 165.0, 156.0, 156.0, 155.0, 162.0, 168.0, 168.0, 168.0, 162.0, 162.0, 162.0, 162.0, 168.0, 168.0, 165.0, 165.0, 165.0, 156.0, 164.0, 156.0, 156.0, 156.0, 156.0, 156.0, 163.0, 163.0, 163.0, 169.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 155.0, 168.0, 155.0, 155.0, 155.0, 155.0, 155.0, 168.0, 156.0, 153.0, 153.0, 153.0, 153.0, 153.0, 161.0, 161.0, 155.0, 155.0, 166.0, 166.0, 165.0, 160.0, 165.0, 155.0, 162.0, 166.0, 163.0, 163.0, 162.0, 162.0, 162.0, 162.0, 162.0, 153.0, 153.0, 153.0, 165.0, 153.0, 153.0, 165.0, 153.0, 153.0, 153.0, 153.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 154.0, 173.0, 173.0, 154.0, 173.0, 154.0, 163.0, 164.0, 164.0, 164.0, 164.0, 160.0, 164.0, 164.0, 164.0, 164.0, 161.0, 158.0, 158.0, 158.0, 161.0, 161.0, 167.0, 161.0, 161.0, 167.0, 161.0, 161.0, 160.0, 165.0, 171.0, 164.0, 167.0, 165.0, 165.0, 167.0, 167.0, 167.0, 167.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 158.0, 158.0, 159.0, 159.0, 164.0, 159.0, 158.0, 158.0, 158.0, 160.0, 160.0, 165.0, 160.0, 165.0, 164.0, 164.0, 164.0, 163.0, 163.0, 163.0, 163.0, 166.0, 165.0, 165.0, 165.0, 165.0, 158.0, 159.0, 164.0, 164.0, 164.0, 159.0, 159.0, 164.0, 164.0, 159.0, 164.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 150.0, 156.0, 150.0, 150.0, 156.0, 150.0, 150.0, 156.0, 157.0, 157.0, 159.0, 160.0, 165.0, 155.0, 155.0, 160.0, 160.0, 164.0, 160.0, 159.0, 164.0, 159.0, 161.0, 161.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 159.0, 159.0, 159.0, 164.0, 158.0, 163.0, 168.0, 163.0, 163.0, 168.0, 168.0, 168.0, 163.0, 165.0, 165.0, 165.0, 165.0, 165.0, 150.0, 150.0, 156.0, 156.0, 150.0, 150.0, 163.0, 158.0, 163.0, 158.0, 158.0, 159.0, 164.0, 164.0, 164.0, 159.0, 159.0, 159.0, 164.0, 164.0, 159.0, 159.0, 160.0, 160.0, 159.0, 154.0, 173.0, 154.0, 173.0, 173.0, 173.0, 171.0, 171.0, 155.0, 155.0, 157.0, 165.0, 165.0, 161.0, 161.0, 151.0, 161.0, 161.0, 161.0, 162.0, 161.0, 162.0, 162.0, 162.0, 162.0, 161.0, 165.0, 160.0, 165.0, 165.0, 164.0, 164.0, 164.0, 170.0, 161.0, 161.0, 170.0, 170.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 165.0, 151.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 171.0, 164.0, 163.0, 156.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 158.0, 158.0, 163.0, 158.0, 158.0, 158.0, 158.0, 153.0, 158.0, 156.0, 157.0, 156.0, 164.0, 164.0, 151.0, 151.0, 164.0, 164.0, 164.0, 163.0, 163.0, 164.0, 163.0, 162.0, 162.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 153.0, 153.0, 156.0, 158.0, 159.0, 158.0, 159.0, 155.0, 155.0, 166.0, 160.0, 165.0, 165.0, 158.0, 163.0, 158.0, 163.0, 163.0, 163.0, 163.0, 163.0, 158.0, 163.0, 163.0, 163.0, 163.0, 165.0, 163.0, 163.0, 165.0, 163.0, 163.0, 163.0, 163.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 159.0, 159.0, 159.0, 162.0, 162.0, 156.0, 156.0, 156.0, 159.0, 159.0, 149.0, 165.0, 149.0, 149.0, 149.0, 149.0, 168.0, 168.0, 149.0, 157.0, 157.0, 165.0, 165.0, 165.0, 160.0, 168.0, 168.0, 168.0, 160.0, 160.0, 163.0, 159.0, 161.0, 156.0, 156.0, 154.0, 156.0, 159.0, 167.0, 159.0, 167.0, 159.0, 159.0, 159.0, 155.0, 155.0, 153.0, 167.0, 161.0, 160.0, 160.0, 162.0, 162.0, 166.0, 162.0, 162.0, 166.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 160.0, 165.0, 159.0, 159.0, 159.0, 159.0, 162.0, 159.0, 159.0, 159.0, 159.0, 159.0, 170.0, 165.0, 160.0, 165.0, 162.0, 164.0, 162.0, 162.0, 164.0, 162.0, 164.0, 164.0, 162.0, 156.0, 156.0, 156.0, 156.0, 166.0, 166.0, 165.0, 166.0, 166.0, 166.0, 166.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 153.0, 156.0, 156.0, 156.0, 161.0, 161.0, 161.0, 161.0, 163.0, 163.0, 163.0, 160.0, 165.0, 160.0, 165.0, 165.0, 159.0, 159.0, 159.0, 165.0, 159.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 157.0, 155.0, 155.0, 150.0, 159.0, 159.0, 163.0, 163.0, 158.0, 163.0, 158.0, 163.0, 163.0, 168.0, 160.0, 164.0, 165.0, 165.0, 165.0, 162.0, 164.0, 162.0, 164.0, 162.0, 162.0, 160.0, 165.0, 165.0, 164.0, 164.0, 164.0, 164.0, 161.0, 161.0, 161.0, 161.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 151.0, 155.0, 159.0, 155.0, 159.0, 159.0, 155.0, 159.0, 159.0, 159.0, 159.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 155.0, 159.0, 159.0, 159.0, 157.0, 151.0, 160.0, 160.0, 160.0, 160.0, 164.0, 160.0, 160.0, 157.0, 157.0, 169.0, 165.0, 160.0, 165.0, 165.0, 165.0, 156.0, 156.0, 156.0, 156.0, 156.0, 164.0, 164.0, 159.0, 159.0, 150.0, 150.0, 162.0, 162.0, 162.0, 162.0, 162.0, 150.0, 164.0, 162.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 158.0, 158.0, 158.0, 165.0, 158.0, 158.0, 172.0, 166.0, 166.0, 166.0, 166.0, 165.0, 160.0, 165.0, 153.0, 153.0, 153.0, 156.0, 153.0, 159.0, 171.0, 171.0, 159.0, 159.0, 167.0, 165.0, 160.0, 160.0, 160.0, 159.0, 152.0, 152.0, 158.0, 158.0, 158.0, 158.0, 163.0, 159.0, 156.0, 168.0, 160.0, 160.0, 164.0, 164.0, 164.0, 164.0, 164.0, 162.0, 168.0, 162.0, 168.0, 162.0, 165.0, 165.0, 153.0, 153.0, 153.0, 153.0, 161.0, 164.0, 164.0, 161.0, 160.0, 163.0, 168.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 155.0, 155.0, 154.0, 166.0, 154.0, 154.0, 157.0, 157.0, 157.0, 157.0, 162.0, 162.0, 157.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 151.0, 159.0, 159.0, 151.0, 151.0, 151.0, 159.0, 159.0, 159.0, 151.0, 159.0, 160.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 157.0, 154.0, 154.0, 159.0, 159.0, 159.0, 163.0, 163.0, 162.0, 162.0, 162.0, 162.0, 165.0, 150.0, 150.0, 150.0, 150.0, 150.0, 156.0, 158.0, 174.0, 163.0, 170.0, 163.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 162.0, 162.0, 164.0, 165.0, 165.0, 165.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 161.0, 162.0, 162.0, 161.0, 162.0, 162.0, 161.0, 162.0, 161.0, 155.0, 154.0, 171.0, 164.0, 164.0, 150.0, 150.0, 164.0, 150.0, 155.0, 154.0, 166.0, 166.0, 166.0, 166.0, 166.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 165.0, 165.0, 158.0, 158.0, 158.0, 163.0, 158.0, 163.0, 165.0, 165.0, 165.0, 165.0, 164.0, 160.0, 160.0, 160.0, 155.0, 155.0, 159.0, 159.0, 159.0, 159.0, 155.0, 155.0, 159.0, 155.0, 159.0, 165.0, 159.0, 154.0, 156.0, 165.0, 165.0, 156.0, 156.0, 159.0, 165.0, 159.0, 165.0, 162.0, 160.0, 159.0, 159.0, 154.0, 154.0, 173.0, 154.0, 173.0, 159.0, 150.0, 150.0, 150.0, 161.0, 150.0, 150.0, 150.0, 150.0, 161.0, 161.0, 153.0, 160.0, 160.0, 165.0, 160.0, 165.0, 157.0, 157.0, 157.0, 162.0, 162.0, 162.0, 168.0, 168.0, 168.0, 161.0, 161.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 154.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 164.0, 164.0, 164.0, 164.0, 160.0, 157.0, 157.0, 165.0, 163.0, 160.0, 166.0, 164.0, 164.0, 159.0, 155.0, 156.0, 161.0, 153.0, 161.0, 161.0, 153.0, 161.0, 153.0, 161.0, 153.0, 161.0, 161.0, 168.0, 165.0, 165.0, 165.0, 160.0, 165.0, 171.0, 171.0, 171.0, 171.0, 171.0, 171.0, 161.0, 171.0, 171.0, 158.0, 159.0, 164.0, 164.0, 159.0, 164.0, 164.0, 169.0, 171.0, 161.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 154.0, 154.0, 159.0, 162.0, 162.0, 154.0, 154.0, 162.0, 154.0, 154.0, 154.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 173.0, 154.0, 154.0, 154.0, 173.0, 154.0, 173.0, 154.0, 152.0, 152.0, 152.0, 153.0, 153.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 150.0, 150.0, 156.0, 150.0, 163.0, 158.0, 158.0, 163.0, 163.0, 158.0, 158.0, 165.0, 160.0, 159.0, 152.0, 152.0, 152.0, 165.0, 165.0, 152.0, 165.0, 152.0, 152.0, 152.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 159.0, 159.0, 159.0, 155.0, 159.0, 155.0, 155.0, 159.0, 159.0, 163.0, 163.0, 163.0, 163.0, 167.0, 165.0, 155.0, 168.0, 168.0, 155.0, 165.0, 162.0, 157.0, 161.0, 161.0, 157.0, 161.0, 165.0, 165.0, 164.0, 164.0, 164.0, 164.0, 153.0, 153.0, 153.0, 156.0, 166.0, 166.0, 165.0, 166.0, 166.0, 166.0, 165.0, 160.0, 164.0, 164.0, 164.0, 171.0, 164.0, 171.0, 171.0, 171.0, 164.0, 171.0, 165.0, 151.0, 155.0, 159.0, 155.0, 155.0, 155.0, 155.0, 159.0, 159.0, 159.0, 155.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 159.0, 155.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 157.0, 164.0, 164.0, 160.0, 165.0, 160.0, 165.0, 163.0, 163.0, 163.0, 160.0, 162.0, 160.0, 162.0, 165.0, 160.0, 164.0, 157.0, 157.0, 157.0, 157.0, 164.0, 164.0, 164.0, 157.0, 156.0, 161.0, 161.0, 161.0, 161.0, 166.0, 161.0, 166.0, 166.0, 166.0, 166.0, 160.0, 154.0, 160.0, 160.0, 159.0, 159.0, 152.0, 152.0, 152.0, 160.0, 152.0, 159.0, 159.0, 159.0, 158.0, 162.0, 162.0, 155.0, 159.0, 159.0, 159.0, 159.0, 161.0, 155.0, 155.0, 155.0, 161.0, 155.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 163.0, 166.0, 166.0, 166.0, 165.0, 165.0, 160.0, 160.0, 160.0, 166.0, 160.0, 160.0, 166.0, 166.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 155.0, 160.0, 155.0, 155.0, 165.0, 157.0, 165.0, 157.0, 165.0, 161.0, 164.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 162.0, 162.0, 164.0, 162.0, 162.0, 165.0, 160.0, 165.0, 165.0, 160.0, 164.0, 171.0, 171.0, 171.0, 171.0, 171.0, 163.0, 165.0, 160.0, 165.0, 157.0, 157.0, 157.0, 164.0, 159.0, 173.0, 154.0, 154.0, 169.0, 166.0, 164.0, 164.0, 156.0, 164.0, 153.0, 153.0, 153.0, 157.0, 157.0, 149.0, 163.0, 168.0, 168.0, 168.0, 168.0, 161.0, 161.0, 171.0, 171.0, 171.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 151.0, 160.0, 159.0, 159.0, 159.0, 159.0, 160.0, 160.0, 160.0, 160.0, 159.0, 160.0, 165.0, 165.0, 171.0, 161.0, 171.0, 171.0, 161.0, 171.0, 161.0, 171.0, 165.0, 160.0, 165.0, 158.0, 163.0, 158.0, 158.0, 163.0, 163.0, 158.0, 163.0, 156.0, 160.0, 161.0, 164.0, 161.0, 159.0, 162.0, 165.0, 165.0, 165.0, 162.0, 160.0, 165.0, 160.0, 165.0, 165.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 168.0, 164.0, 158.0, 162.0, 162.0, 158.0, 162.0, 165.0, 157.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 164.0, 150.0, 150.0, 150.0, 150.0, 163.0, 163.0, 163.0, 163.0, 164.0, 167.0, 167.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 162.0, 162.0, 164.0, 164.0, 165.0, 165.0, 159.0, 155.0, 160.0, 164.0, 164.0, 163.0, 159.0, 159.0, 160.0, 165.0, 158.0, 164.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 164.0, 157.0, 157.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 156.0, 156.0, 156.0, 154.0, 154.0, 154.0, 154.0, 167.0, 167.0, 154.0, 167.0, 154.0, 154.0, 167.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 153.0, 153.0, 153.0, 156.0, 156.0, 153.0, 156.0, 153.0, 153.0, 153.0, 156.0, 153.0, 156.0, 154.0, 159.0, 154.0, 159.0, 154.0, 154.0, 154.0, 159.0, 159.0, 165.0, 165.0, 160.0, 157.0, 165.0, 165.0, 165.0, 165.0, 159.0, 155.0, 155.0, 163.0, 155.0, 155.0, 156.0, 160.0, 160.0, 170.0, 170.0, 162.0, 157.0, 163.0, 163.0, 169.0, 163.0, 162.0, 160.0, 160.0, 162.0, 162.0, 160.0, 160.0]\n",
            "Best num_ones with epsilon greedy=0.8 is 149.0\n",
            "[165.0, 159.0, 160.0, 163.0, 155.0, 156.0, 166.0, 168.0, 166.0, 168.0, 166.0, 166.0, 166.0, 166.0, 168.0, 160.0, 151.0, 151.0, 162.0, 162.0, 162.0, 165.0, 165.0, 155.0, 165.0, 163.0, 165.0, 160.0, 165.0, 165.0, 166.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 166.0, 160.0, 165.0, 160.0, 157.0, 156.0, 165.0, 170.0, 170.0, 170.0, 170.0, 170.0, 170.0, 170.0, 165.0, 164.0, 160.0, 157.0, 157.0, 163.0, 169.0, 167.0, 167.0, 167.0, 167.0, 167.0, 163.0, 155.0, 156.0, 163.0, 163.0, 163.0, 156.0, 156.0, 163.0, 163.0, 163.0, 156.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 156.0, 164.0, 156.0, 156.0, 155.0, 161.0, 171.0, 171.0, 161.0, 161.0, 171.0, 171.0, 161.0, 171.0, 161.0, 173.0, 173.0, 173.0, 154.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 154.0, 173.0, 158.0, 156.0, 160.0, 164.0, 171.0, 171.0, 171.0, 169.0, 169.0, 167.0, 169.0, 169.0, 169.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 153.0, 153.0, 156.0, 168.0, 155.0, 155.0, 154.0, 154.0, 154.0, 154.0, 154.0, 160.0, 164.0, 160.0, 160.0, 162.0, 162.0, 162.0, 160.0, 162.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 164.0, 161.0, 161.0, 164.0, 164.0, 165.0, 160.0, 160.0, 165.0, 165.0, 164.0, 158.0, 158.0, 158.0, 158.0, 159.0, 164.0, 159.0, 164.0, 159.0, 159.0, 164.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 159.0, 159.0, 154.0, 154.0, 158.0, 158.0, 169.0, 158.0, 169.0, 169.0, 169.0, 158.0, 169.0, 160.0, 165.0, 160.0, 151.0, 151.0, 168.0, 159.0, 159.0, 156.0, 156.0, 162.0, 164.0, 156.0, 164.0, 165.0, 160.0, 162.0, 162.0, 158.0, 168.0, 158.0, 157.0, 156.0, 156.0, 157.0, 166.0, 160.0, 166.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 163.0, 163.0, 165.0, 165.0, 165.0, 163.0, 163.0, 157.0, 164.0, 157.0, 164.0, 157.0, 157.0, 157.0, 157.0, 157.0, 164.0, 164.0, 165.0, 151.0, 155.0, 155.0, 159.0, 155.0, 159.0, 159.0, 163.0, 156.0, 156.0, 153.0, 165.0, 165.0, 165.0, 160.0, 158.0, 159.0, 159.0, 164.0, 159.0, 159.0, 159.0, 164.0, 159.0, 164.0, 163.0, 165.0, 160.0, 165.0, 165.0, 159.0, 159.0, 151.0, 151.0, 163.0, 161.0, 161.0, 161.0, 161.0, 162.0, 161.0, 161.0, 161.0, 158.0, 158.0, 158.0, 165.0, 158.0, 158.0, 164.0, 154.0, 164.0, 165.0, 159.0, 163.0, 163.0, 159.0, 169.0, 169.0, 164.0, 158.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 168.0, 163.0, 151.0, 159.0, 159.0, 169.0, 169.0, 159.0, 163.0, 168.0, 168.0, 163.0, 158.0, 160.0, 160.0, 165.0, 162.0, 164.0, 162.0, 162.0, 164.0, 164.0, 164.0, 165.0, 159.0, 159.0, 165.0, 159.0, 159.0, 159.0, 165.0, 161.0, 154.0, 158.0, 154.0, 154.0, 158.0, 154.0, 158.0, 154.0, 154.0, 154.0, 154.0, 158.0, 164.0, 164.0, 164.0, 158.0, 163.0, 163.0, 163.0, 163.0, 163.0, 158.0, 159.0, 159.0, 159.0, 162.0, 159.0, 159.0, 159.0, 159.0, 162.0, 159.0, 157.0, 164.0, 164.0, 169.0, 169.0, 164.0, 164.0, 164.0, 164.0, 164.0, 158.0, 160.0, 165.0, 160.0, 154.0, 173.0, 154.0, 173.0, 154.0, 173.0, 161.0, 151.0, 151.0, 162.0, 162.0, 151.0, 158.0, 163.0, 153.0, 153.0, 156.0, 156.0, 160.0, 165.0, 165.0, 162.0, 162.0, 165.0, 165.0, 165.0, 165.0, 153.0, 153.0, 153.0, 167.0, 167.0, 153.0, 153.0, 156.0, 156.0, 156.0, 159.0, 160.0, 164.0, 157.0, 157.0, 157.0, 157.0, 154.0, 161.0, 161.0, 160.0, 160.0, 161.0, 160.0, 160.0, 160.0, 160.0, 163.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 158.0, 158.0, 156.0, 156.0, 156.0, 156.0, 157.0, 157.0, 162.0, 162.0, 158.0, 166.0, 166.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 164.0, 168.0, 155.0, 156.0, 156.0, 156.0, 156.0, 156.0, 173.0, 173.0, 173.0, 173.0, 156.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 160.0, 165.0, 159.0, 154.0, 154.0, 173.0, 154.0, 173.0, 154.0, 153.0, 165.0, 154.0, 154.0, 154.0, 163.0, 164.0, 164.0, 164.0, 164.0, 164.0, 159.0, 159.0, 159.0, 171.0, 171.0, 171.0, 165.0, 164.0, 164.0, 164.0, 157.0, 166.0, 157.0, 157.0, 157.0, 166.0, 157.0, 157.0, 157.0, 156.0, 156.0, 156.0, 167.0, 156.0, 170.0, 159.0, 170.0, 159.0, 159.0, 158.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 153.0, 167.0, 167.0, 166.0, 166.0, 156.0, 156.0, 156.0, 164.0, 155.0, 155.0, 158.0, 158.0, 158.0, 158.0, 159.0, 164.0, 159.0, 164.0, 171.0, 166.0, 166.0, 165.0, 160.0, 165.0, 160.0, 160.0, 165.0, 165.0, 157.0, 157.0, 164.0, 164.0, 159.0, 159.0, 159.0, 164.0, 159.0, 164.0, 161.0, 160.0, 160.0, 165.0, 159.0, 159.0, 154.0, 154.0, 159.0, 159.0, 159.0, 159.0, 159.0, 155.0, 165.0, 165.0, 157.0, 165.0, 165.0, 160.0, 160.0, 159.0, 161.0, 161.0, 161.0, 159.0, 158.0, 168.0, 156.0, 156.0, 156.0, 161.0, 164.0, 161.0, 164.0, 164.0, 164.0, 164.0, 164.0, 165.0, 160.0, 164.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 165.0, 165.0, 159.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 159.0, 155.0, 155.0, 166.0, 166.0, 166.0, 166.0, 166.0, 166.0, 162.0, 160.0, 162.0, 165.0, 158.0, 165.0, 158.0, 165.0, 165.0, 165.0, 157.0, 161.0, 161.0, 157.0, 157.0, 164.0, 164.0, 164.0, 166.0, 166.0, 164.0, 159.0, 151.0, 151.0, 151.0, 159.0, 159.0, 159.0, 154.0, 162.0, 154.0, 162.0, 162.0, 160.0, 163.0, 155.0, 155.0, 159.0, 159.0, 151.0, 151.0, 151.0, 151.0, 162.0, 151.0, 151.0, 162.0, 162.0, 162.0, 162.0, 159.0, 151.0, 151.0, 151.0, 159.0, 151.0, 151.0, 151.0, 151.0, 159.0, 151.0, 151.0, 157.0, 164.0, 159.0, 159.0, 159.0, 164.0, 159.0, 164.0, 164.0, 164.0, 164.0, 165.0, 160.0, 165.0, 159.0, 154.0, 154.0, 173.0, 173.0, 154.0, 154.0, 168.0, 168.0, 168.0, 164.0, 161.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 164.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 157.0, 157.0, 163.0, 169.0, 169.0, 163.0, 169.0, 169.0, 169.0, 163.0, 169.0, 152.0, 159.0, 152.0, 152.0, 161.0, 161.0, 159.0, 159.0, 162.0, 162.0, 153.0, 159.0, 165.0, 159.0, 165.0, 161.0, 155.0, 163.0, 161.0, 162.0, 161.0, 161.0, 163.0, 161.0, 161.0, 161.0, 157.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 151.0, 166.0, 162.0, 166.0, 166.0, 162.0, 162.0, 162.0, 162.0, 162.0, 166.0, 160.0, 160.0, 160.0, 160.0, 165.0, 153.0, 156.0, 153.0, 154.0, 154.0, 165.0, 154.0, 161.0, 161.0, 164.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 159.0, 151.0, 151.0, 159.0, 151.0, 159.0, 161.0, 161.0, 159.0, 159.0, 161.0, 159.0, 165.0, 160.0, 165.0, 159.0, 159.0, 164.0, 164.0, 154.0, 173.0, 154.0, 154.0, 159.0, 159.0, 154.0, 173.0, 165.0, 156.0, 157.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 171.0, 166.0, 165.0, 166.0, 157.0, 157.0, 164.0, 160.0, 160.0, 162.0, 160.0, 160.0, 162.0, 169.0, 163.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 151.0, 159.0, 151.0, 151.0, 159.0, 159.0, 151.0, 151.0, 159.0, 159.0, 159.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 159.0, 159.0, 161.0, 172.0, 161.0, 172.0, 156.0, 165.0, 156.0, 165.0, 156.0, 156.0, 165.0, 156.0, 162.0, 161.0, 161.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 155.0, 159.0, 155.0, 155.0, 155.0, 155.0, 155.0, 155.0, 158.0, 158.0, 158.0, 165.0, 165.0, 153.0, 153.0, 156.0, 164.0, 164.0, 164.0, 164.0, 164.0, 165.0, 165.0, 150.0, 156.0, 150.0, 152.0, 160.0, 160.0, 165.0, 165.0, 156.0, 156.0, 156.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 155.0, 162.0, 162.0, 162.0, 162.0, 160.0, 159.0, 159.0, 159.0, 162.0, 161.0, 155.0, 155.0, 159.0, 159.0, 159.0, 160.0, 161.0, 161.0, 160.0, 160.0, 160.0, 158.0, 162.0, 162.0, 150.0, 159.0, 165.0, 165.0, 159.0, 165.0, 159.0, 165.0, 165.0, 159.0, 159.0, 160.0, 165.0, 165.0, 157.0, 157.0, 157.0, 157.0, 157.0, 157.0, 164.0, 157.0, 157.0, 164.0, 164.0, 165.0, 165.0, 165.0, 157.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 157.0, 162.0, 160.0, 152.0, 163.0, 152.0, 152.0, 163.0, 163.0, 163.0, 163.0, 152.0, 152.0, 163.0, 155.0, 161.0, 157.0, 161.0, 157.0, 161.0, 157.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 159.0, 155.0, 168.0, 168.0, 166.0, 168.0, 168.0, 168.0, 168.0, 168.0, 166.0, 168.0, 153.0, 156.0, 164.0, 164.0, 156.0, 154.0, 159.0, 156.0, 159.0, 159.0, 159.0, 160.0, 160.0, 164.0, 167.0, 163.0, 169.0, 163.0, 169.0, 169.0, 157.0, 157.0, 157.0, 157.0, 158.0, 165.0, 162.0, 162.0, 162.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 155.0, 160.0, 155.0, 160.0, 160.0, 155.0, 160.0, 157.0, 163.0, 161.0, 166.0, 160.0, 165.0, 160.0, 160.0, 165.0, 156.0, 156.0, 164.0, 164.0, 164.0, 158.0, 160.0, 160.0, 160.0, 158.0, 160.0, 160.0, 160.0, 165.0, 160.0, 151.0, 155.0, 159.0, 155.0, 159.0, 159.0, 155.0, 159.0, 159.0, 159.0, 155.0, 165.0, 160.0, 165.0, 160.0, 165.0, 153.0, 157.0, 157.0, 169.0, 169.0, 169.0, 157.0, 156.0, 156.0, 156.0, 170.0, 160.0, 160.0, 162.0, 162.0, 157.0, 157.0, 166.0, 157.0, 157.0, 157.0, 164.0, 166.0, 166.0, 166.0, 164.0, 165.0, 165.0, 165.0, 165.0, 151.0, 151.0, 151.0, 151.0, 155.0, 155.0, 159.0, 159.0, 159.0, 159.0, 159.0, 156.0, 156.0, 164.0, 161.0, 168.0, 168.0, 168.0, 164.0, 164.0, 155.0, 155.0, 161.0, 156.0, 156.0, 164.0, 164.0, 164.0, 156.0, 158.0, 156.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 153.0, 156.0, 153.0, 153.0, 156.0, 156.0, 156.0, 153.0, 157.0, 153.0, 153.0, 165.0, 160.0, 165.0, 165.0, 160.0, 158.0, 155.0, 155.0, 155.0, 152.0, 152.0, 156.0, 171.0, 171.0, 169.0, 168.0, 169.0, 163.0, 168.0, 167.0, 165.0, 165.0, 160.0, 165.0, 158.0, 165.0, 161.0, 157.0, 161.0, 157.0, 157.0, 161.0, 157.0, 157.0, 157.0, 157.0, 162.0, 162.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 164.0, 164.0, 165.0, 156.0, 164.0, 156.0, 157.0, 166.0, 159.0, 159.0, 150.0, 150.0, 150.0, 159.0, 166.0, 166.0, 168.0, 159.0, 164.0, 159.0, 159.0, 160.0, 151.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 159.0, 164.0, 164.0, 164.0, 160.0, 165.0, 158.0, 158.0, 158.0, 156.0, 157.0, 157.0, 157.0, 160.0, 157.0, 160.0, 160.0, 160.0, 157.0, 157.0, 151.0, 155.0, 159.0, 159.0, 155.0, 159.0, 159.0, 160.0, 158.0, 158.0, 158.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 159.0, 164.0, 159.0, 160.0, 163.0, 163.0, 163.0, 160.0, 159.0, 154.0, 153.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 166.0, 159.0, 159.0, 153.0, 167.0, 167.0, 167.0, 167.0, 167.0, 153.0, 158.0, 165.0, 153.0, 163.0, 159.0, 155.0, 159.0, 159.0, 155.0, 155.0, 165.0, 165.0, 165.0, 165.0, 165.0, 153.0, 165.0, 161.0, 161.0, 155.0, 161.0, 161.0, 161.0, 161.0, 155.0, 161.0, 165.0, 165.0, 160.0, 160.0, 151.0, 159.0, 165.0, 163.0, 167.0, 162.0, 162.0, 159.0, 159.0, 157.0, 157.0, 158.0, 158.0, 158.0, 165.0, 165.0, 165.0, 165.0, 153.0, 158.0, 158.0, 158.0, 153.0, 153.0, 158.0, 153.0, 153.0, 153.0, 163.0, 153.0, 160.0, 160.0, 152.0, 152.0, 152.0, 153.0, 153.0, 152.0, 152.0, 152.0, 162.0, 154.0, 159.0, 160.0, 165.0, 171.0, 164.0, 171.0, 171.0, 164.0, 158.0, 154.0, 164.0, 164.0, 164.0, 154.0, 154.0, 154.0, 154.0, 154.0, 164.0, 164.0, 165.0, 165.0, 160.0, 165.0, 156.0, 156.0, 159.0, 159.0, 151.0, 151.0, 151.0, 151.0, 160.0, 160.0, 165.0, 165.0, 158.0, 161.0, 161.0, 160.0, 160.0, 165.0, 160.0, 161.0, 161.0, 161.0, 154.0, 173.0, 173.0, 154.0, 154.0, 173.0, 154.0, 173.0, 173.0, 154.0, 154.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 160.0, 151.0, 155.0, 155.0, 159.0, 155.0, 155.0, 155.0, 159.0, 159.0, 155.0, 159.0, 165.0]\n",
            "Best num_ones with epsilon greedy=0.9 is 150.0\n",
            "[165.0, 153.0, 167.0, 153.0, 153.0, 156.0, 157.0, 156.0, 157.0, 156.0, 157.0, 156.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 162.0, 164.0, 153.0, 155.0, 161.0, 164.0, 164.0, 164.0, 161.0, 161.0, 161.0, 164.0, 164.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 161.0, 161.0, 171.0, 171.0, 161.0, 159.0, 159.0, 163.0, 163.0, 172.0, 161.0, 161.0, 172.0, 172.0, 161.0, 172.0, 158.0, 158.0, 165.0, 165.0, 168.0, 160.0, 161.0, 161.0, 166.0, 166.0, 161.0, 165.0, 160.0, 158.0, 159.0, 156.0, 160.0, 156.0, 160.0, 155.0, 155.0, 163.0, 155.0, 163.0, 163.0, 167.0, 167.0, 167.0, 165.0, 165.0, 165.0, 160.0, 161.0, 164.0, 161.0, 161.0, 164.0, 161.0, 161.0, 164.0, 164.0, 164.0, 159.0, 154.0, 154.0, 154.0, 154.0, 154.0, 159.0, 154.0, 154.0, 154.0, 159.0, 159.0, 160.0, 164.0, 162.0, 164.0, 162.0, 164.0, 164.0, 162.0, 162.0, 164.0, 164.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 173.0, 154.0, 173.0, 164.0, 164.0, 157.0, 159.0, 159.0, 157.0, 166.0, 166.0, 157.0, 165.0, 171.0, 161.0, 171.0, 161.0, 154.0, 154.0, 159.0, 154.0, 154.0, 155.0, 154.0, 153.0, 153.0, 162.0, 157.0, 157.0, 166.0, 162.0, 166.0, 157.0, 157.0, 157.0, 156.0, 164.0, 156.0, 164.0, 164.0, 156.0, 164.0, 161.0, 161.0, 159.0, 159.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 161.0, 161.0, 161.0, 164.0, 156.0, 154.0, 154.0, 154.0, 162.0, 161.0, 161.0, 155.0, 161.0, 161.0, 161.0, 161.0, 159.0, 151.0, 164.0, 164.0, 156.0, 156.0, 156.0, 164.0, 163.0, 163.0, 160.0, 163.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 160.0, 161.0, 171.0, 161.0, 161.0, 156.0, 165.0, 165.0, 156.0, 156.0, 156.0, 165.0, 165.0, 165.0, 157.0, 154.0, 154.0, 154.0, 154.0, 157.0, 157.0, 154.0, 154.0, 154.0, 159.0, 166.0, 163.0, 165.0, 164.0, 154.0, 154.0, 154.0, 154.0, 162.0, 154.0, 167.0, 167.0, 158.0, 154.0, 156.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 164.0, 171.0, 171.0, 164.0, 171.0, 164.0, 164.0, 171.0, 165.0, 160.0, 160.0, 164.0, 164.0, 170.0, 165.0, 166.0, 169.0, 169.0, 165.0, 163.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 160.0, 160.0, 160.0, 159.0, 152.0, 167.0, 152.0, 160.0, 160.0, 160.0, 160.0, 153.0, 160.0, 160.0, 153.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 159.0, 159.0, 164.0, 164.0, 171.0, 164.0, 160.0, 160.0, 166.0, 162.0, 162.0, 160.0, 165.0, 163.0, 159.0, 162.0, 164.0, 159.0, 161.0, 161.0, 161.0, 161.0, 158.0, 158.0, 158.0, 158.0, 158.0, 157.0, 157.0, 157.0, 164.0, 157.0, 158.0, 154.0, 154.0, 154.0, 154.0, 154.0, 162.0, 154.0, 154.0, 162.0, 154.0, 154.0, 160.0, 165.0, 165.0, 165.0, 160.0, 154.0, 154.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 166.0, 156.0, 161.0, 165.0, 153.0, 153.0, 153.0, 153.0, 158.0, 158.0, 153.0, 161.0, 167.0, 161.0, 167.0, 164.0, 165.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 153.0, 157.0, 157.0, 153.0, 153.0, 165.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 164.0, 169.0, 164.0, 155.0, 155.0, 154.0, 154.0, 160.0, 155.0, 159.0, 173.0, 173.0, 156.0, 152.0, 152.0, 152.0, 152.0, 166.0, 166.0, 166.0, 152.0, 171.0, 171.0, 171.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 159.0, 151.0, 164.0, 157.0, 157.0, 157.0, 164.0, 164.0, 170.0, 170.0, 170.0, 170.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 153.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 156.0, 154.0, 156.0, 156.0, 160.0, 164.0, 164.0, 165.0, 164.0, 164.0, 166.0, 166.0, 166.0, 159.0, 166.0, 166.0, 166.0, 159.0, 159.0, 166.0, 166.0, 159.0, 159.0, 166.0, 165.0, 158.0, 165.0, 158.0, 165.0, 158.0, 158.0, 158.0, 162.0, 159.0, 162.0, 162.0, 165.0, 164.0, 164.0, 164.0, 164.0, 164.0, 156.0, 162.0, 171.0, 171.0, 171.0, 171.0, 162.0, 162.0, 171.0, 162.0, 164.0, 160.0, 161.0, 165.0, 165.0, 154.0, 166.0, 166.0, 154.0, 154.0, 166.0, 166.0, 166.0, 166.0, 154.0, 154.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 158.0, 161.0, 158.0, 161.0, 161.0, 158.0, 161.0, 158.0, 158.0, 156.0, 151.0, 156.0, 156.0, 151.0, 159.0, 159.0, 162.0, 162.0, 169.0, 169.0, 166.0, 165.0, 171.0, 164.0, 171.0, 171.0, 164.0, 171.0, 171.0, 164.0, 171.0, 171.0, 171.0, 170.0, 160.0, 165.0, 160.0, 165.0, 160.0, 160.0, 163.0, 152.0, 163.0, 152.0, 152.0, 159.0, 168.0, 157.0, 161.0, 161.0, 163.0, 163.0, 160.0, 151.0, 151.0, 151.0, 154.0, 154.0, 154.0, 159.0, 154.0, 154.0, 159.0, 159.0, 165.0, 159.0, 159.0, 165.0, 159.0, 166.0, 158.0, 162.0, 162.0, 162.0, 158.0, 161.0, 157.0, 159.0, 161.0, 159.0, 159.0, 166.0, 151.0, 166.0, 169.0, 169.0, 157.0, 169.0, 169.0, 157.0, 169.0, 167.0, 160.0, 165.0, 165.0, 164.0, 173.0, 167.0, 164.0, 160.0, 160.0, 161.0, 161.0, 161.0, 157.0, 158.0, 158.0, 158.0, 160.0, 158.0, 158.0, 158.0, 158.0, 161.0, 161.0, 158.0, 158.0, 159.0, 159.0, 159.0, 164.0, 165.0, 165.0, 165.0, 165.0, 162.0, 157.0, 165.0, 165.0, 162.0, 162.0, 160.0, 160.0, 160.0, 161.0, 163.0, 163.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 159.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 159.0, 160.0, 160.0, 155.0, 155.0, 155.0, 160.0, 155.0, 155.0, 155.0, 155.0, 153.0, 155.0, 153.0, 153.0, 153.0, 153.0, 155.0, 156.0, 156.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 153.0, 159.0, 152.0, 162.0, 162.0, 156.0, 162.0, 163.0, 160.0, 160.0, 160.0, 160.0, 160.0, 173.0, 154.0, 154.0, 173.0, 173.0, 154.0, 173.0, 173.0, 173.0, 154.0, 173.0, 154.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 155.0, 159.0, 155.0, 159.0, 155.0, 159.0, 155.0, 155.0, 161.0, 161.0, 161.0, 165.0, 165.0, 163.0, 163.0, 162.0, 160.0, 162.0, 162.0, 160.0, 162.0, 162.0, 163.0, 160.0, 162.0, 162.0, 160.0, 163.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 164.0, 159.0, 159.0, 159.0, 159.0, 159.0, 159.0, 164.0, 164.0, 159.0, 159.0, 164.0, 159.0, 159.0, 151.0, 151.0, 151.0, 151.0, 159.0, 151.0, 159.0, 151.0, 159.0, 159.0, 150.0, 160.0, 160.0, 160.0, 154.0, 157.0, 158.0, 157.0, 153.0, 159.0, 159.0, 152.0, 159.0, 159.0, 159.0, 163.0, 163.0, 163.0, 166.0, 166.0, 167.0, 167.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 160.0, 160.0, 160.0, 160.0, 165.0, 164.0, 163.0, 163.0, 157.0, 162.0, 165.0, 165.0, 165.0, 165.0, 157.0, 165.0, 166.0, 159.0, 166.0, 160.0, 165.0, 160.0, 160.0, 165.0, 159.0, 164.0, 164.0, 159.0, 159.0, 166.0, 159.0, 166.0, 166.0, 166.0, 163.0, 162.0, 164.0, 164.0, 164.0, 164.0, 153.0, 153.0, 153.0, 153.0, 153.0, 157.0, 160.0, 160.0, 159.0, 168.0, 159.0, 160.0, 165.0, 157.0, 160.0, 164.0, 160.0, 160.0, 151.0, 151.0, 151.0, 151.0, 153.0, 153.0, 153.0, 164.0, 153.0, 164.0, 153.0, 160.0, 165.0, 160.0, 154.0, 150.0, 154.0, 164.0, 161.0, 161.0, 167.0, 168.0, 167.0, 168.0, 167.0, 167.0, 159.0, 155.0, 172.0, 168.0, 168.0, 172.0, 161.0, 172.0, 172.0, 161.0, 172.0, 161.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 155.0, 172.0, 163.0, 172.0, 166.0, 157.0, 157.0, 157.0, 166.0, 161.0, 155.0, 165.0, 165.0, 151.0, 151.0, 151.0, 151.0, 151.0, 155.0, 159.0, 159.0, 157.0, 166.0, 157.0, 165.0, 165.0, 173.0, 154.0, 173.0, 173.0, 154.0, 157.0, 157.0, 157.0, 157.0, 157.0, 154.0, 157.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 165.0, 160.0, 158.0, 158.0, 159.0, 159.0, 159.0, 164.0, 159.0, 159.0, 161.0, 161.0, 166.0, 160.0, 160.0, 165.0, 162.0, 162.0, 164.0, 153.0, 169.0, 169.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 158.0, 169.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 159.0, 164.0, 164.0, 164.0, 164.0, 159.0, 154.0, 160.0, 160.0, 160.0, 160.0, 156.0, 160.0, 156.0, 161.0, 161.0, 161.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 159.0, 159.0, 161.0, 164.0, 164.0, 164.0, 163.0, 165.0, 171.0, 171.0, 164.0, 160.0, 164.0, 162.0, 162.0, 161.0, 162.0, 159.0, 159.0, 163.0, 159.0, 157.0, 165.0, 169.0, 165.0, 165.0, 169.0, 165.0, 165.0, 165.0, 165.0, 169.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 160.0, 160.0, 160.0, 155.0, 159.0, 155.0, 159.0, 161.0, 161.0, 161.0, 157.0, 159.0, 164.0, 151.0, 151.0, 166.0, 168.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 161.0, 165.0, 165.0, 160.0, 157.0, 157.0, 157.0, 165.0, 161.0, 162.0, 162.0, 156.0, 156.0, 163.0, 163.0, 172.0, 163.0, 163.0, 164.0, 164.0, 163.0, 166.0, 160.0, 160.0, 165.0, 157.0, 164.0, 164.0, 159.0, 164.0, 164.0, 159.0, 159.0, 159.0, 164.0, 158.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 164.0, 159.0, 159.0, 166.0, 156.0, 150.0, 156.0, 165.0, 165.0, 165.0, 156.0, 165.0, 157.0, 157.0, 160.0, 161.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 160.0, 160.0, 164.0, 166.0, 166.0, 160.0, 160.0, 160.0, 165.0, 160.0, 153.0, 163.0, 168.0, 158.0, 158.0, 168.0, 168.0, 168.0, 168.0, 168.0, 168.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 151.0, 151.0, 151.0, 151.0, 151.0, 155.0, 155.0, 155.0, 159.0, 155.0, 159.0, 160.0, 160.0, 165.0, 159.0, 159.0, 159.0, 164.0, 164.0, 166.0, 154.0, 166.0, 166.0, 166.0, 154.0, 154.0, 155.0, 157.0, 162.0, 158.0, 162.0, 165.0, 160.0, 160.0, 165.0, 150.0, 150.0, 159.0, 152.0, 161.0, 163.0, 161.0, 161.0, 161.0, 162.0, 157.0, 165.0, 160.0, 160.0, 165.0, 157.0, 164.0, 159.0, 164.0, 169.0, 164.0, 165.0, 159.0, 159.0, 159.0, 162.0, 165.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 171.0, 161.0, 168.0, 169.0, 169.0, 164.0, 164.0, 164.0, 170.0, 171.0, 171.0, 171.0, 165.0, 160.0, 155.0, 155.0, 155.0, 159.0, 155.0, 159.0, 160.0, 159.0, 159.0, 159.0, 159.0, 153.0, 153.0, 153.0, 150.0, 164.0, 164.0, 161.0, 161.0, 161.0, 153.0, 151.0, 164.0, 151.0, 151.0, 165.0, 159.0, 160.0, 160.0, 160.0, 157.0, 157.0, 157.0, 157.0, 163.0, 157.0, 153.0, 153.0, 153.0, 153.0, 165.0, 153.0, 165.0, 153.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 165.0, 165.0, 165.0, 159.0, 159.0, 159.0, 159.0, 159.0, 163.0, 163.0, 158.0, 164.0, 161.0, 157.0, 158.0, 158.0, 152.0, 160.0, 160.0, 160.0, 152.0, 152.0, 152.0, 157.0, 154.0, 154.0, 154.0, 165.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 166.0, 166.0, 159.0, 158.0, 158.0, 168.0, 158.0, 158.0, 168.0, 168.0, 168.0, 168.0, 158.0, 168.0, 155.0, 154.0, 162.0, 157.0, 161.0, 161.0, 157.0, 161.0, 161.0, 160.0, 161.0, 161.0, 151.0, 155.0, 155.0, 159.0, 159.0, 155.0, 155.0, 159.0, 159.0, 155.0, 155.0, 160.0, 158.0, 161.0, 161.0, 162.0, 162.0, 162.0, 154.0, 154.0, 162.0, 165.0, 152.0, 152.0, 153.0, 161.0, 161.0, 158.0, 160.0, 162.0, 162.0, 168.0, 168.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 165.0, 160.0, 160.0, 160.0, 165.0, 163.0, 160.0, 166.0, 162.0, 162.0, 166.0, 157.0, 160.0, 160.0, 160.0, 165.0, 160.0, 160.0, 160.0, 165.0, 162.0, 162.0, 155.0, 160.0, 155.0, 158.0, 159.0, 162.0, 159.0, 162.0, 159.0, 162.0, 159.0, 160.0, 165.0, 160.0, 165.0, 165.0, 160.0, 165.0, 160.0, 160.0, 165.0, 165.0, 165.0, 160.0, 165.0, 165.0, 157.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 163.0, 160.0, 160.0, 165.0, 157.0, 163.0, 163.0, 163.0, 169.0, 169.0, 169.0, 163.0, 163.0, 163.0, 160.0, 165.0, 160.0, 165.0, 165.0, 159.0, 162.0, 158.0, 168.0, 168.0, 158.0, 158.0, 154.0, 167.0, 159.0, 167.0, 159.0, 167.0, 167.0, 159.0, 159.0, 167.0, 154.0, 164.0, 164.0, 164.0, 164.0, 164.0, 165.0, 160.0, 157.0, 157.0, 157.0, 163.0, 169.0, 162.0, 164.0, 164.0, 159.0, 159.0, 158.0, 160.0, 160.0, 165.0, 171.0, 171.0, 164.0, 171.0, 164.0, 149.0, 160.0, 160.0, 161.0, 161.0, 161.0, 159.0, 161.0, 161.0, 161.0, 161.0, 165.0, 160.0, 160.0, 160.0, 165.0, 165.0, 150.0, 156.0, 156.0, 150.0, 150.0, 150.0, 160.0, 160.0, 158.0, 160.0, 160.0, 165.0, 160.0, 160.0, 165.0, 165.0, 160.0, 160.0, 165.0, 160.0, 165.0, 164.0, 164.0, 165.0, 160.0, 164.0, 167.0, 154.0, 163.0, 154.0, 163.0, 154.0, 154.0, 163.0, 163.0, 163.0, 154.0, 164.0, 164.0, 165.0, 156.0, 157.0, 159.0, 155.0, 159.0, 155.0, 151.0, 151.0, 150.0, 150.0, 150.0, 164.0, 163.0, 163.0, 158.0, 163.0, 158.0, 163.0, 158.0, 158.0, 157.0, 163.0, 160.0, 160.0, 160.0, 162.0, 161.0, 161.0, 162.0, 162.0, 162.0, 165.0]\n",
            "Best num_ones with epsilon greedy=1.0 is 149.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-5\n",
        "\n",
        "W = 8\n",
        "N = 10\n",
        "vec = [ i for i in range(2 ** W)]\n",
        "random.shuffle(vec)\n",
        "num_ones = []\n",
        "environment = CauchyEnv(W, N, vec, num_ones)\n",
        "tf_env = tf_py_environment.TFPyEnvironment(environment)\n",
        "\n",
        "eval_num_ones = []\n",
        "eval_environment = CauchyEnv(W, N, vec, eval_num_ones)\n",
        "tf_eval_env = tf_py_environment.TFPyEnvironment(eval_environment)\n",
        "\n",
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(tf_env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    tf_env.time_step_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate))\n",
        "\n",
        "replay_buffer_capacity = 10000\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    batch_size=tf_env.batch_size,\n",
        "    max_length=replay_buffer_capacity)\n",
        "replay_buffer.clear()\n",
        "\n",
        "# Add an observer that adds to the replay buffer:\n",
        "replay_observer = [replay_buffer.add_batch]\n",
        "\n",
        "collect_steps_per_iteration = 1000\n",
        "collect_op = dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "  tf_env,\n",
        "  agent.collect_policy,\n",
        "  observers=replay_observer,\n",
        "  num_episodes=collect_steps_per_iteration).run()\n",
        "\n",
        "print(num_ones)\n",
        "print(f\"Best num_ones = {np.min(num_ones)}\")"
      ],
      "metadata": {
        "id": "3gWc157yFm6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent.collect_policy._epsilon)"
      ],
      "metadata": {
        "id": "p3UtF9Jb_7H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_min_ones(environment, policy, num_ones, num_episodes=10):\n",
        "  total_num_ones = 0\n",
        "  actions = []\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = environment.reset()\n",
        "    num_ones.clear()\n",
        "    actions.clear()\n",
        "    rewards_list = []\n",
        "    rewards = 0\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      rewards_list = np.concatenate((rewards_list, time_step.reward.numpy()))\n",
        "      rewards += time_step.reward\n",
        "      actions.append(action_step.action.numpy())\n",
        "    print(actions)\n",
        "    print(num_ones)\n",
        "    print(rewards.numpy())\n",
        "    print(rewards_list)\n",
        "    total_num_ones += np.min(num_ones)\n",
        "  return total_num_ones / num_episodes"
      ],
      "metadata": {
        "id": "jXtLQeJubp6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_interval = 100\n",
        "eval_interval = 100\n",
        "\n",
        "num_train_steps = 100\n",
        "df = pd.DataFrame()\n",
        "\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=10,\n",
        "    num_steps=2)\n",
        "iterator = iter(dataset)\n",
        "\n",
        "for step in range(num_train_steps):\n",
        "  trajectories, _ = next(iterator)\n",
        "  train_loss = agent.train(experience=trajectories).loss\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "  if step % eval_interval == 0:\n",
        "    min_ones = compute_min_ones(tf_eval_env, agent.collect_policy, eval_num_ones)\n",
        "    print('step = {0}: min_ones = {1}'.format(step, min_ones))\n",
        "  df = pd.concat([df, pd.DataFrame({'step': [step], 'min_ones': [min_ones], 'loss': [train_loss.numpy()]})])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "Z9UmdfKlUgWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent.policy)"
      ],
      "metadata": {
        "id": "huqkN22qd7wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index('step', inplace=True)\n",
        "df['min_ones'].plot()"
      ],
      "metadata": {
        "id": "yyILya5RRyJb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}